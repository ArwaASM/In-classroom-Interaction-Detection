{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "mount_file_id": "1iSbE3TT5AGe_RLdJqFoY5tat3BqmX8Dh",
      "authorship_tag": "ABX9TyO3oEGGSMsdelZbMTfOcY8T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArwaASM/TPE/blob/main/YOLOv8l_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Checking GPU Access**"
      ],
      "metadata": {
        "id": "_LvrjdNZg8Bq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pG81Zjnka1G0",
        "outputId": "2130fd40-012c-4eec-f35a-4919b0ba9c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov  3 00:29:52 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPHiEGd_cOeq",
        "outputId": "ca0a34d9-5bd3-4387-bdeb-2b7b7afa5717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install YOLOv8**"
      ],
      "metadata": {
        "id": "ui-h5vrXg-iP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pip install method (recommended)\n",
        "\n",
        "!pip install ultralytics==8.0.196\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuZzJSZpcWQC",
        "outputId": "24ccdb42-e756-40e2-903b-d2e60557a05f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.196 üöÄ Python-3.10.12 torch-2.5.0+cu121 CUDA:0 (NVIDIA A100-SXM4-40GB, 40514MiB)\n",
            "Setup complete ‚úÖ (12 CPUs, 83.5 GB RAM, 32.3/235.7 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "from IPython.display import display, Image"
      ],
      "metadata": {
        "id": "MuvfEO0ZcZQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/datasets\n",
        "%cd /content/datasets\n",
        "\n",
        "!pip install roboflow --quiet\n",
        "\n",
        "!pip install roboflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tfGUccfccnD",
        "outputId": "1591cb36-73d5-44cc-a3da-2013de4b181b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.48)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.8.30)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (10.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.2.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.6)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.3.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.54.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Imort our Dataset**"
      ],
      "metadata": {
        "id": "uj9RAJi2hKas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"Tp9HSxuOcXKVDZCz5***\")\n",
        "project = rf.workspace(\"arwa-almubarak-yiboc\").project(\"students-behaviors-detection-wkavr\")\n",
        "version = project.version(4)\n",
        "dataset = version.download(\"yolov8\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SofF_AJFciyL",
        "outputId": "3dd94d34-6285-4582-cea2-1db5d917184b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.48)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.8.30)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (10.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.2.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.6)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.3.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.54.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.4.0)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in students-behaviors-detection-4 to yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82276/82276 [00:06<00:00, 13631.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to students-behaviors-detection-4 in yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14554/14554 [00:01<00:00, 9074.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Configuration**"
      ],
      "metadata": {
        "id": "BLtOJbPVhTF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U albumentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5AfqQBlcjfV",
        "outputId": "63d72496-7c35-44b1-8a54-e5e424836c9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.4.21-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Collecting albucore==0.0.20 (from albumentations)\n",
            "  Downloading albucore-0.0.20-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.20->albumentations) (3.10.6)\n",
            "Collecting simsimd>=5.9.2 (from albucore==0.0.20->albumentations)\n",
            "  Downloading simsimd-5.9.10-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (54 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n",
            "Downloading albumentations-1.4.21-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m227.9/227.9 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albucore-0.0.20-py3-none-any.whl (12 kB)\n",
            "Downloading simsimd-5.9.10-cp310-cp310-manylinux_2_28_x86_64.whl (680 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m680.7/680.7 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: simsimd, albucore, albumentations\n",
            "  Attempting uninstall: albucore\n",
            "    Found existing installation: albucore 0.0.19\n",
            "    Uninstalling albucore-0.0.19:\n",
            "      Successfully uninstalled albucore-0.0.19\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.4.20\n",
            "    Uninstalling albumentations-1.4.20:\n",
            "      Successfully uninstalled albumentations-1.4.20\n",
            "Successfully installed albucore-0.0.20 albumentations-1.4.21 simsimd-5.9.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GCyQyzwVcxAx",
        "outputId": "66d77bda-c4c6-4846-ba27-06536a6be5ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.0.196)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.27-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.10-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.27-py3-none-any.whl (878 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m879.0/879.0 kB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.10-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: ultralytics\n",
            "    Found existing installation: ultralytics 8.0.196\n",
            "    Uninstalling ultralytics-8.0.196:\n",
            "      Successfully uninstalled ultralytics-8.0.196\n",
            "Successfully installed ultralytics-8.3.27 ultralytics-thop-2.0.10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "thop",
                  "ultralytics"
                ]
              },
              "id": "4e5f725f41634ea689993ac8f68616bb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Training**"
      ],
      "metadata": {
        "id": "oMA4sZqXhYRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "\n",
        "!yolo task=detect mode=train model=yolov8l.pt data=/content/datasets/students-behaviors-detection-4/data.yaml epochs=120 imgsz=800 plots=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KzCeLx5e27Q",
        "outputId": "8696c772-7d12-4683-80e8-b8607ff8bc96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8l.pt to 'yolov8l.pt'...\n",
            "100% 83.7M/83.7M [00:00<00:00, 301MB/s]\n",
            "Ultralytics 8.3.27 üöÄ Python-3.10.12 torch-2.5.0+cu121 CUDA:0 (NVIDIA A100-SXM4-40GB, 40514MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8l.pt, data=/content/datasets/students-behaviors-detection-4/data.yaml, epochs=120, time=None, patience=100, batch=16, imgsz=800, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100% 755k/755k [00:00<00:00, 94.1MB/s]\n",
            "Overriding model.yaml nc=80 with nc=11\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
            "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
            "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
            "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
            "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
            " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
            " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
            " 22        [15, 18, 21]  1   5591281  ultralytics.nn.modules.head.Detect           [11, [256, 512, 512]]         \n",
            "Model summary: 365 layers, 43,638,321 parameters, 43,638,305 gradients, 165.4 GFLOPs\n",
            "\n",
            "Transferred 589/595 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n",
            "100% 5.35M/5.35M [00:00<00:00, 50.9MB/s]\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/students-behaviors-detection-4/train/labels... 6369 images, 108 backgrounds, 0 corrupt: 100% 6369/6369 [00:05<00:00, 1181.60it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/students-behaviors-detection-4/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/students-behaviors-detection-4/valid/labels... 450 images, 15 backgrounds, 0 corrupt: 100% 450/450 [00:00<00:00, 1018.70it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/students-behaviors-detection-4/valid/labels.cache\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n",
            "Image sizes 800 train, 800 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 120 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      1/120      16.1G      1.745      2.426      1.884          3        800: 100% 399/399 [01:43<00:00,  3.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:04<00:00,  3.26it/s]\n",
            "                   all        450       1849      0.546      0.544      0.517      0.289\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      2/120      16.1G      1.496      1.391       1.59          5        800: 100% 399/399 [01:38<00:00,  4.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.55it/s]\n",
            "                   all        450       1849      0.551      0.619       0.62      0.347\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      3/120      16.1G      1.518      1.353      1.598          2        800: 100% 399/399 [01:37<00:00,  4.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.61it/s]\n",
            "                   all        450       1849      0.455      0.528      0.514       0.27\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      4/120      15.8G      1.548      1.411      1.641          2        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.65it/s]\n",
            "                   all        450       1849      0.524      0.339      0.341      0.176\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      5/120        16G      1.507      1.307      1.616          9        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.61it/s]\n",
            "                   all        450       1849       0.55       0.55      0.584      0.298\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      6/120      16.1G      1.459      1.206      1.583         15        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.61it/s]\n",
            "                   all        450       1849      0.537      0.637      0.642      0.352\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      7/120      16.1G      1.433      1.155       1.56         14        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.67it/s]\n",
            "                   all        450       1849      0.593      0.634      0.645       0.36\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      8/120      15.9G      1.409      1.105      1.549          5        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.66it/s]\n",
            "                   all        450       1849      0.629      0.676      0.712      0.417\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      9/120      16.1G      1.378       1.06      1.534          5        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.51it/s]\n",
            "                   all        450       1849      0.649      0.697        0.7      0.404\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     10/120      16.1G       1.36      1.018      1.521          6        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.60it/s]\n",
            "                   all        450       1849      0.763      0.688      0.776      0.441\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     11/120      16.1G      1.334     0.9793      1.506          8        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.61it/s]\n",
            "                   all        450       1849      0.655      0.754      0.745      0.436\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     12/120      15.9G      1.315     0.9542      1.482          1        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.60it/s]\n",
            "                   all        450       1849      0.734      0.699       0.76       0.45\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     13/120      16.1G      1.299     0.9134      1.467         11        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.64it/s]\n",
            "                   all        450       1849      0.741      0.735      0.794      0.484\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     14/120        16G      1.275     0.8915       1.45          2        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.66it/s]\n",
            "                   all        450       1849      0.743      0.738        0.8        0.5\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     15/120      15.9G      1.262     0.8759      1.449          2        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.62it/s]\n",
            "                   all        450       1849      0.687      0.761      0.764      0.465\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     16/120      15.9G      1.244     0.8543      1.433         23        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.70it/s]\n",
            "                   all        450       1849      0.717      0.725      0.781      0.483\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     17/120        16G      1.227       0.83       1.43         21        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.53it/s]\n",
            "                   all        450       1849      0.717      0.776      0.775      0.485\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     18/120        16G      1.215     0.8034       1.41         13        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.69it/s]\n",
            "                   all        450       1849      0.723      0.755      0.794      0.487\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     19/120      16.1G      1.199     0.8011      1.405          4        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.68it/s]\n",
            "                   all        450       1849      0.711      0.781      0.784      0.477\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     20/120      15.9G       1.19     0.7864      1.406          3        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.64it/s]\n",
            "                   all        450       1849      0.829      0.757      0.834      0.511\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     21/120      16.1G      1.169      0.763      1.386          4        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.69it/s]\n",
            "                   all        450       1849      0.715      0.794      0.777      0.473\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     22/120        16G      1.163      0.756      1.387          4        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.69it/s]\n",
            "                   all        450       1849      0.727      0.821      0.822        0.5\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     23/120      16.1G       1.14     0.7379      1.372          8        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.60it/s]\n",
            "                   all        450       1849      0.782      0.772      0.804      0.502\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     24/120      15.9G      1.134     0.7274      1.373          8        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.68it/s]\n",
            "                   all        450       1849      0.717      0.786      0.803      0.507\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     25/120      16.1G      1.129     0.7237      1.359         12        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.57it/s]\n",
            "                   all        450       1849      0.737      0.818      0.817      0.501\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     26/120        16G      1.108     0.7037      1.344          3        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.68it/s]\n",
            "                   all        450       1849      0.751      0.769      0.813      0.485\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     27/120      16.1G      1.099     0.6983      1.337         14        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.65it/s]\n",
            "                   all        450       1849      0.747        0.8      0.814      0.511\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     28/120      15.9G      1.087     0.6825      1.335         13        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.69it/s]\n",
            "                   all        450       1849      0.774      0.756       0.82      0.521\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     29/120      16.1G      1.069     0.6664      1.316          5        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.59it/s]\n",
            "                   all        450       1849      0.768      0.768      0.825      0.523\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     30/120        16G      1.061     0.6647      1.311          7        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.63it/s]\n",
            "                   all        450       1849      0.761      0.767      0.804      0.499\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     31/120      16.1G      1.051     0.6497      1.301          2        800: 100% 399/399 [01:36<00:00,  4.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.58it/s]\n",
            "                   all        450       1849      0.797      0.794      0.852      0.548\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     32/120      15.8G      1.039     0.6411      1.302          6        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.62it/s]\n",
            "                   all        450       1849      0.768      0.819      0.812      0.504\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     33/120      16.1G      1.036     0.6369        1.3         11        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.69it/s]\n",
            "                   all        450       1849      0.806      0.783      0.832      0.519\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     34/120      16.1G      1.018     0.6228      1.271          5        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.57it/s]\n",
            "                   all        450       1849      0.772       0.75      0.794      0.502\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     35/120      16.1G      1.008     0.6191      1.269          9        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.66it/s]\n",
            "                   all        450       1849      0.748      0.774      0.825       0.52\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     36/120      15.8G      1.003     0.6163      1.269          4        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.61it/s]\n",
            "                   all        450       1849      0.702      0.806      0.808      0.506\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     37/120      16.1G     0.9901     0.6083      1.269          3        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.67it/s]\n",
            "                   all        450       1849      0.783      0.756      0.816      0.522\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     38/120        16G     0.9808     0.6006       1.27          8        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.70it/s]\n",
            "                   all        450       1849      0.758      0.821      0.833      0.528\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     39/120      16.1G     0.9676      0.589      1.239         12        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.66it/s]\n",
            "                   all        450       1849      0.752      0.801       0.82       0.52\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     40/120      15.9G     0.9639     0.5876       1.24          5        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.71it/s]\n",
            "                   all        450       1849      0.772      0.813       0.83      0.523\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     41/120      16.1G     0.9484     0.5761      1.229          8        800: 100% 399/399 [01:35<00:00,  4.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.71it/s]\n",
            "                   all        450       1849      0.769      0.788      0.832      0.529\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     42/120        16G     0.9428     0.5738      1.231          2        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.65it/s]\n",
            "                   all        450       1849      0.772       0.82      0.834       0.51\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     43/120      16.1G     0.9315     0.5628      1.215          6        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.69it/s]\n",
            "                   all        450       1849      0.754      0.804      0.838      0.528\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     44/120      15.9G     0.9222     0.5575      1.205          6        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.69it/s]\n",
            "                   all        450       1849      0.774      0.791      0.834      0.529\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     45/120        16G     0.9195     0.5556      1.211         14        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.67it/s]\n",
            "                   all        450       1849       0.76      0.794       0.82      0.522\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     46/120        16G     0.9123     0.5499      1.211          4        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.62it/s]\n",
            "                   all        450       1849      0.753      0.811      0.839      0.534\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     47/120      16.1G     0.9006     0.5402      1.198         10        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.72it/s]\n",
            "                   all        450       1849      0.785       0.78       0.83      0.534\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     48/120      15.9G     0.8912      0.539      1.193         11        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.64it/s]\n",
            "                   all        450       1849      0.803      0.727      0.828      0.533\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     49/120      16.1G      0.882     0.5245      1.182          9        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.69it/s]\n",
            "                   all        450       1849      0.749       0.79      0.826       0.53\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     50/120        16G     0.8772     0.5212      1.187          7        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.63it/s]\n",
            "                   all        450       1849      0.804      0.762      0.829      0.528\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     51/120      16.1G     0.8691     0.5139      1.187          2        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.65it/s]\n",
            "                   all        450       1849      0.771      0.783      0.833      0.539\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     52/120      15.9G     0.8638     0.5124      1.181          9        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.63it/s]\n",
            "                   all        450       1849      0.772      0.777      0.843      0.536\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     53/120      16.1G     0.8562     0.5107      1.176         10        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.63it/s]\n",
            "                   all        450       1849      0.818      0.762      0.838      0.537\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     54/120      16.1G     0.8454      0.499      1.165         10        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.68it/s]\n",
            "                   all        450       1849       0.79      0.812      0.853      0.553\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     55/120      16.1G     0.8433     0.5053      1.154          4        800: 100% 399/399 [01:35<00:00,  4.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.72it/s]\n",
            "                   all        450       1849      0.768      0.778      0.826      0.533\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     56/120      15.9G     0.8403     0.5023      1.149          3        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.70it/s]\n",
            "                   all        450       1849       0.77      0.809      0.832      0.525\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     57/120        16G     0.8329     0.4963      1.138          7        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.71it/s]\n",
            "                   all        450       1849      0.768      0.773      0.814      0.523\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     58/120        16G     0.8204     0.4854      1.133          8        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.69it/s]\n",
            "                   all        450       1849      0.772      0.837      0.851      0.548\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     59/120      16.1G     0.8159     0.4841      1.138          6        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.63it/s]\n",
            "                   all        450       1849      0.782      0.777      0.835      0.537\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     60/120      15.9G     0.8094      0.482      1.131         11        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.74it/s]\n",
            "                   all        450       1849      0.782        0.8      0.837      0.538\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     61/120      16.1G     0.8061     0.4813      1.133          8        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.56it/s]\n",
            "                   all        450       1849      0.802      0.812       0.85      0.549\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     62/120      15.9G     0.7977     0.4716      1.129          8        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.70it/s]\n",
            "                   all        450       1849      0.815      0.779      0.829      0.546\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     63/120      16.1G     0.7919     0.4615      1.118         10        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.68it/s]\n",
            "                   all        450       1849      0.809       0.79      0.833      0.534\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     64/120      15.9G     0.7859     0.4619      1.114          2        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.70it/s]\n",
            "                   all        450       1849      0.832      0.763      0.838      0.543\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     65/120        16G      0.778     0.4579      1.118         13        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.66it/s]\n",
            "                   all        450       1849      0.801      0.797      0.844      0.541\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     66/120        16G     0.7718      0.451      1.113         17        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.66it/s]\n",
            "                   all        450       1849      0.803      0.777      0.835      0.544\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     67/120      16.1G     0.7674     0.4537      1.117          8        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.67it/s]\n",
            "                   all        450       1849      0.833      0.747      0.838      0.544\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     68/120      15.9G     0.7609     0.4514      1.117          0        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.69it/s]\n",
            "                   all        450       1849      0.753      0.818      0.845       0.55\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     69/120        16G     0.7492     0.4396      1.105          4        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.68it/s]\n",
            "                   all        450       1849      0.731      0.824      0.837      0.545\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     70/120      16.1G     0.7517     0.4407      1.105          4        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.70it/s]\n",
            "                   all        450       1849      0.828      0.746      0.843      0.541\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     71/120      16.1G     0.7455     0.4388      1.095          2        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.66it/s]\n",
            "                   all        450       1849      0.831      0.745      0.844      0.543\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     72/120      15.9G     0.7355     0.4321      1.083          5        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.67it/s]\n",
            "                   all        450       1849      0.843      0.742      0.841      0.542\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     73/120        16G     0.7282     0.4333      1.086          1        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.74it/s]\n",
            "                   all        450       1849      0.838      0.719      0.839       0.54\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     74/120        16G     0.7231     0.4246      1.089         13        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.61it/s]\n",
            "                   all        450       1849      0.803       0.76      0.823       0.53\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     75/120      16.1G      0.719     0.4174       1.09          9        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.70it/s]\n",
            "                   all        450       1849      0.778      0.784      0.828       0.54\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     76/120      15.9G     0.7162      0.418      1.077         12        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.75it/s]\n",
            "                   all        450       1849      0.766      0.806      0.837      0.546\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     77/120        16G     0.7107     0.4183      1.085          6        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.69it/s]\n",
            "                   all        450       1849      0.814       0.76      0.836      0.544\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     78/120        16G     0.7039     0.4136      1.071         11        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.75it/s]\n",
            "                   all        450       1849      0.772      0.814      0.844      0.547\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     79/120      16.1G      0.698     0.4071      1.065         22        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.68it/s]\n",
            "                   all        450       1849      0.817      0.775      0.839      0.542\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     80/120      15.9G     0.6846     0.3986      1.062          8        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.73it/s]\n",
            "                   all        450       1849      0.831      0.772      0.834      0.544\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     81/120      16.1G     0.6838     0.3937      1.053         13        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.69it/s]\n",
            "                   all        450       1849      0.792      0.799      0.834      0.541\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     82/120        16G     0.6822     0.3996      1.056          6        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.58it/s]\n",
            "                   all        450       1849      0.817      0.769      0.837      0.539\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     83/120      16.1G     0.6727     0.3945      1.048          4        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.75it/s]\n",
            "                   all        450       1849      0.795      0.791      0.833      0.541\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     84/120      15.9G     0.6708      0.393      1.049          8        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.73it/s]\n",
            "                   all        450       1849       0.82      0.772      0.843      0.552\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     85/120        16G     0.6635     0.3883      1.048         11        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.51it/s]\n",
            "                   all        450       1849      0.827       0.76      0.826       0.54\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     86/120        16G     0.6635     0.3906      1.039          4        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.69it/s]\n",
            "                   all        450       1849      0.815       0.77      0.835      0.542\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     87/120      16.1G     0.6546     0.3799      1.035         10        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.63it/s]\n",
            "                   all        450       1849      0.789      0.783      0.834      0.543\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     88/120      15.9G     0.6452     0.3756      1.032         13        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.73it/s]\n",
            "                   all        450       1849       0.84      0.761      0.841      0.547\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     89/120        16G     0.6443     0.3772      1.031         11        800: 100% 399/399 [01:35<00:00,  4.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.71it/s]\n",
            "                   all        450       1849      0.796      0.793      0.839      0.546\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     90/120        16G     0.6351     0.3683      1.027          5        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.72it/s]\n",
            "                   all        450       1849      0.879      0.731      0.835      0.543\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     91/120        16G      0.635     0.3671      1.023          5        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.68it/s]\n",
            "                   all        450       1849      0.863      0.735      0.837      0.546\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     92/120      15.9G      0.629     0.3674      1.019         12        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.75it/s]\n",
            "                   all        450       1849      0.879      0.731      0.842       0.55\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     93/120        16G     0.6244     0.3641      1.017          8        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.68it/s]\n",
            "                   all        450       1849      0.848      0.744      0.838      0.549\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     94/120        16G     0.6211     0.3608      1.019          9        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.67it/s]\n",
            "                   all        450       1849      0.798      0.791      0.844      0.551\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     95/120      16.1G     0.6127     0.3542      1.017         10        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.57it/s]\n",
            "                   all        450       1849      0.817      0.788       0.85      0.551\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     96/120      15.9G     0.6087     0.3589      1.014          3        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.69it/s]\n",
            "                   all        450       1849      0.814      0.783      0.846       0.55\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     97/120      16.1G     0.6058     0.3519      1.018          4        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.74it/s]\n",
            "                   all        450       1849      0.819      0.772      0.836      0.543\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     98/120        16G     0.5969     0.3506       1.01         15        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.70it/s]\n",
            "                   all        450       1849      0.834      0.758      0.836       0.54\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     99/120      16.1G     0.5895     0.3452      1.008          3        800: 100% 399/399 [01:36<00:00,  4.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.75it/s]\n",
            "                   all        450       1849      0.775      0.824      0.838      0.546\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    100/120      15.9G     0.5869      0.349      1.002          3        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.67it/s]\n",
            "                   all        450       1849      0.803      0.795      0.842      0.547\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    101/120        16G     0.5872     0.3417      1.001          5        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.73it/s]\n",
            "                   all        450       1849      0.778      0.811      0.838      0.546\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    102/120        16G     0.5784      0.335     0.9943          5        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.74it/s]\n",
            "                   all        450       1849      0.793      0.802      0.839       0.55\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    103/120      16.1G     0.5752     0.3354     0.9954          8        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.61it/s]\n",
            "                   all        450       1849        0.8       0.78      0.835      0.548\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    104/120      15.9G     0.5702     0.3351      0.992          6        800: 100% 399/399 [01:36<00:00,  4.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.70it/s]\n",
            "                   all        450       1849      0.861      0.744      0.838      0.548\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    105/120        16G     0.5627     0.3307     0.9905          2        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.65it/s]\n",
            "                   all        450       1849      0.801      0.786      0.836      0.548\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    106/120        16G     0.5612      0.327     0.9946          3        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.63it/s]\n",
            "                   all        450       1849      0.805      0.786      0.833      0.548\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    107/120      16.1G     0.5538     0.3263     0.9889         13        800: 100% 399/399 [01:36<00:00,  4.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.71it/s]\n",
            "                   all        450       1849      0.796      0.793      0.835      0.546\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    108/120      15.8G     0.5474     0.3222     0.9823          4        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.69it/s]\n",
            "                   all        450       1849      0.817      0.779      0.832      0.544\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    109/120        16G     0.5463     0.3236     0.9828         13        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.60it/s]\n",
            "                   all        450       1849      0.816      0.777      0.832      0.546\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    110/120      16.1G     0.5399     0.3266     0.9799          1        800: 100% 399/399 [01:36<00:00,  4.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.68it/s]\n",
            "                   all        450       1849      0.807       0.78      0.831      0.543\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    111/120      16.1G     0.4886     0.2316     0.9439          7        800: 100% 399/399 [01:37<00:00,  4.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.54it/s]\n",
            "                   all        450       1849      0.815      0.783      0.835      0.547\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    112/120      15.9G     0.4722     0.2262     0.9357          1        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.61it/s]\n",
            "                   all        450       1849      0.818      0.785      0.836       0.55\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    113/120        16G     0.4622     0.2203     0.9271          2        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.62it/s]\n",
            "                   all        450       1849      0.817      0.784      0.835      0.548\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    114/120      16.1G     0.4565     0.2274     0.9246          3        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.61it/s]\n",
            "                   all        450       1849      0.842      0.768      0.835      0.551\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    115/120      16.1G       0.45      0.216     0.9214          6        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.66it/s]\n",
            "                   all        450       1849       0.84      0.777      0.836      0.551\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    116/120      15.9G     0.4395     0.2215     0.9139          1        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.73it/s]\n",
            "                   all        450       1849      0.836      0.781      0.839      0.551\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    117/120      16.1G     0.4352     0.2099     0.9133          8        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.66it/s]\n",
            "                   all        450       1849      0.829      0.778      0.839       0.55\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    118/120        16G     0.4286     0.2067     0.9074          5        800: 100% 399/399 [01:36<00:00,  4.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.68it/s]\n",
            "                   all        450       1849      0.832       0.78       0.84      0.551\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    119/120      16.1G     0.4239     0.2082     0.9084          6        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.62it/s]\n",
            "                   all        450       1849      0.837      0.775      0.842      0.553\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    120/120      15.9G     0.4218     0.2034     0.9047          8        800: 100% 399/399 [01:36<00:00,  4.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:03<00:00,  4.59it/s]\n",
            "                   all        450       1849      0.837      0.777      0.842      0.552\n",
            "\n",
            "120 epochs completed in 3.364 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 87.7MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 87.7MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics 8.3.27 üöÄ Python-3.10.12 torch-2.5.0+cu121 CUDA:0 (NVIDIA A100-SXM4-40GB, 40514MiB)\n",
            "Model summary (fused): 268 layers, 43,615,089 parameters, 0 gradients, 164.9 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 15/15 [00:04<00:00,  3.04it/s]\n",
            "                   all        450       1849      0.789      0.812      0.853      0.553\n",
            "           Closed-Book        171        296      0.817      0.888      0.903      0.558\n",
            "       Electronic-Book          5         12      0.893      0.698      0.928       0.41\n",
            "               No-Book        167        312      0.782      0.875      0.873      0.569\n",
            "           Opened-Book        272        619      0.875      0.871       0.92      0.597\n",
            "          Raising-Hand        119        239      0.846      0.841      0.891      0.509\n",
            "       Student-Answers         36         39       0.87      0.769      0.864      0.565\n",
            "         Student-Reads         54         69      0.627      0.507       0.58      0.416\n",
            "        Student-Writes         37         60      0.736       0.75      0.764      0.496\n",
            "      Teacher-Explains         65         65      0.842      0.903      0.944      0.705\n",
            "Teacher-Follows-up-Students          7          7      0.622          1      0.857      0.708\n",
            "             Worksheet         84        131      0.769      0.832      0.862      0.551\n",
            "Speed: 0.2ms preprocess, 3.5ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n",
            "üí° Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visulaize Training Results**"
      ],
      "metadata": {
        "id": "VPBcB8Hsj3wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "Image(filename=f'{HOME}/runs/detect/train/confusion_matrix.png', width=600)"
      ],
      "metadata": {
        "id": "LlN5ZUyFj3ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "Image(filename=f'{HOME}/runs/detect/train/results.png', width=600)"
      ],
      "metadata": {
        "id": "bz8oQFV6kGdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "Image(filename=f'{HOME}/runs/detect/train/val_batch0_pred.jpg', width=600)"
      ],
      "metadata": {
        "id": "7AdUoRSVkSLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Validation**"
      ],
      "metadata": {
        "id": "pVNTL1hYhhAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "\n",
        "!yolo task=detect mode=val model=/content/runs/detect/train/weights/best.pt data=/content/datasets/students-behaviors-detection-4/data.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5FaAS6lEIAG",
        "outputId": "f8637834-6fea-4754-dd48-7625ed17e230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Ultralytics 8.3.27 üöÄ Python-3.10.12 torch-2.5.0+cu121 CUDA:0 (NVIDIA A100-SXM4-40GB, 40514MiB)\n",
            "Model summary (fused): 268 layers, 43,615,089 parameters, 0 gradients, 164.9 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/students-behaviors-detection-4/valid/labels.cache... 450 images, 15 backgrounds, 0 corrupt: 100% 450/450 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 29/29 [00:06<00:00,  4.44it/s]\n",
            "                   all        450       1849      0.789      0.812      0.853      0.554\n",
            "           Closed-Book        171        296      0.817      0.888      0.903      0.559\n",
            "       Electronic-Book          5         12      0.893      0.698      0.928      0.418\n",
            "               No-Book        167        312      0.781      0.875      0.874      0.568\n",
            "           Opened-Book        272        619      0.874      0.871       0.92      0.595\n",
            "          Raising-Hand        119        239      0.849      0.841      0.891       0.51\n",
            "       Student-Answers         36         39       0.87      0.769      0.864      0.565\n",
            "         Student-Reads         54         69      0.627      0.507       0.58      0.415\n",
            "        Student-Writes         37         60      0.736       0.75      0.764      0.496\n",
            "      Teacher-Explains         65         65      0.842      0.903      0.944      0.704\n",
            "Teacher-Follows-up-Students          7          7      0.621          1      0.857      0.708\n",
            "             Worksheet         84        131      0.769      0.832      0.862      0.552\n",
            "Speed: 0.9ms preprocess, 5.6ms inference, 0.0ms loss, 4.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
            "üí° Learn more at https://docs.ultralytics.com/modes/val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Inference**"
      ],
      "metadata": {
        "id": "ALjIGSEekyWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!yolo task=detect mode=predict model=/content/runs/detect/train/weights/best.pt conf=0.5 source=/content/datasets/students-behaviors-detection-4/test/images save=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXfX8zCDPWFb",
        "outputId": "7ad08ddb-9e76-48c2-b560-8fcc459ac6c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Ultralytics 8.3.27 üöÄ Python-3.10.12 torch-2.5.0+cu121 CUDA:0 (NVIDIA A100-SXM4-40GB, 40514MiB)\n",
            "Model summary (fused): 268 layers, 43,615,089 parameters, 0 gradients, 164.9 GFLOPs\n",
            "\n",
            "image 1/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0010_jpg.rf.20a99cec7de825bc38f7f75bf2904c21.jpg: 800x800 4 No-Books, 2 Opened-Books, 1 Teacher-Explains, 15.8ms\n",
            "image 2/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0017_jpg.rf.8d1e62e0299c5fcc5e33e70ea6df3f16.jpg: 800x800 1 Opened-Book, 1 Teacher-Explains, 11.5ms\n",
            "image 3/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0027_jpg.rf.80ca7d714fa4164c2d63ae5414f09f17.jpg: 800x800 4 No-Books, 2 Opened-Books, 1 Teacher-Explains, 11.1ms\n",
            "image 4/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0040_jpg.rf.908476d825252a8ea97df9a088a08410.jpg: 800x800 1 No-Book, 1 Teacher-Follows-up-Students, 10.7ms\n",
            "image 5/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0076_jpg.rf.1e11ec3995f55e529c4adb0a8a1ea158.jpg: 800x800 3 No-Books, 4 Opened-Books, 10.6ms\n",
            "image 6/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0096_jpg.rf.846c195c082870a3c461d7af3c017902.jpg: 800x800 4 No-Books, 1 Opened-Book, 1 Teacher-Explains, 11.1ms\n",
            "image 7/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0102_jpg.rf.9d121c95a458c1d132b297bd15c5d925.jpg: 800x800 4 Opened-Books, 1 Teacher-Explains, 11.0ms\n",
            "image 8/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0111_jpg.rf.f763fb6b7dd70dcfefa6bfe29dd8a7b3.jpg: 800x800 4 Opened-Books, 10.7ms\n",
            "image 9/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0138_jpg.rf.9e49daa23a1d9dfb269cf4f344051a21.jpg: 800x800 1 No-Book, 2 Opened-Books, 10.6ms\n",
            "image 10/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0190_jpg.rf.3dbbb9adb9bb2c0624a2c76a62afd4cd.jpg: 800x800 1 Closed-Book, 1 Student-Writes, 10.5ms\n",
            "image 11/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0207_jpg.rf.e5c77b9a0570264cc174ed7d891f72a1.jpg: 800x800 2 Closed-Books, 1 Opened-Book, 2 Worksheets, 10.5ms\n",
            "image 12/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0215_jpg.rf.cbe91a88fa9b4ceaee05e9fb5beb9313.jpg: 800x800 2 Closed-Books, 2 Worksheets, 10.5ms\n",
            "image 13/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0216_jpg.rf.abb075bc8d395bc24ea9301bb43e5456.jpg: 800x800 2 Closed-Books, 2 Worksheets, 13.3ms\n",
            "image 14/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0217_jpg.rf.1c60f017710d579cf9d44e9ed775fda7.jpg: 800x800 1 Worksheet, 10.9ms\n",
            "image 15/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0225_jpg.rf.afd0c8f65923acbe483b12717d58b71d.jpg: 800x800 2 Closed-Books, 1 Teacher-Follows-up-Students, 2 Worksheets, 10.7ms\n",
            "image 16/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0232_jpg.rf.76b5a425de42438b3ef0c2ab46a99f67.jpg: 800x800 2 Closed-Books, 2 Worksheets, 10.6ms\n",
            "image 17/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0233_jpg.rf.291c83649b97487fb2c73353f239bb93.jpg: 800x800 1 Closed-Book, 2 Worksheets, 10.6ms\n",
            "image 18/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0250_jpg.rf.3a57fccd5eb2a0c405380610b60b29a8.jpg: 800x800 4 No-Books, 11.1ms\n",
            "image 19/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0281_jpg.rf.07a254f432bf3bed62dacc2e56ed4d14.jpg: 800x800 2 No-Books, 2 Opened-Books, 3 Raising-Hands, 1 Teacher-Explains, 10.6ms\n",
            "image 20/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0282_jpg.rf.7ddd719cd01e975e3172031f183aa506.jpg: 800x800 4 Closed-Books, 1 No-Book, 2 Raising-Hands, 10.8ms\n",
            "image 21/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0288_jpg.rf.a8ebeeafdc17e3a0a92a16d0122c81e1.jpg: 800x800 1 Closed-Book, 1 No-Book, 1 Opened-Book, 1 Student-Answers, 1 Teacher-Explains, 11.0ms\n",
            "image 22/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0291_jpg.rf.dde48a0ddb366a60513906f1572ae60c.jpg: 800x800 1 Closed-Book, 2 No-Books, 1 Opened-Book, 1 Student-Answers, 1 Teacher-Explains, 10.9ms\n",
            "image 23/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_07_39_IMG_1266_MOV-0308_jpg.rf.cafe2dae84a5e6fbf5c8802d8f7283dd.jpg: 800x800 1 Closed-Book, 2 No-Books, 1 Opened-Book, 2 Teacher-Explainss, 11.0ms\n",
            "image 24/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0011_jpg.rf.d974b6c529ce64eaef057c1ad972d48d.jpg: 800x800 2 No-Books, 1 Teacher-Explains, 10.7ms\n",
            "image 25/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0018_jpg.rf.0b0e116b0f1e12a2966e49761ae50d70.jpg: 800x800 1 No-Book, 10.6ms\n",
            "image 26/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0028_jpg.rf.34f8d23efacef34112b2841848c374b5.jpg: 800x800 3 No-Books, 1 Teacher-Explains, 10.7ms\n",
            "image 27/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0030_jpg.rf.ec1d2c6f7b8a7326e8f954799e80c9e9.jpg: 800x800 2 No-Books, 10.7ms\n",
            "image 28/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0042_jpg.rf.db69d1108b8ae8cbe9ab4657e2d50ed2.jpg: 800x800 3 No-Books, 10.8ms\n",
            "image 29/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0046_jpg.rf.dc31724633b6f60c9cbf7a1169b25457.jpg: 800x800 2 No-Books, 10.7ms\n",
            "image 30/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0048_jpg.rf.8dc55a8152f16301c0abf5dba0e4d9e2.jpg: 800x800 6 No-Books, 1 Teacher-Explains, 10.8ms\n",
            "image 31/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0071_jpg.rf.8c687bfd61fddb945138758160682bb4.jpg: 800x800 4 No-Books, 10.7ms\n",
            "image 32/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0077_jpg.rf.9d2a7156a70a78d9910d0f6a493c22b7.jpg: 800x800 4 No-Books, 6 Raising-Hands, 11.1ms\n",
            "image 33/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0088_jpg.rf.64d64ace4243a4d732f38cff23c863cf.jpg: 800x800 3 No-Books, 1 Teacher-Explains, 10.6ms\n",
            "image 34/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0131_jpg.rf.041633ec0e47f0e9a622a4f0ecc0fae5.jpg: 800x800 6 No-Books, 10.5ms\n",
            "image 35/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0135_jpg.rf.4ec20ebc65d9f53a1930151f9cd888d3.jpg: 800x800 3 No-Books, 10.4ms\n",
            "image 36/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0146_jpg.rf.8e2e407eee5521fbeedc1a6d4a33aee8.jpg: 800x800 3 No-Books, 10.4ms\n",
            "image 37/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0155_jpg.rf.542bd36dfa694b6cd0e0301f741444cb.jpg: 800x800 2 No-Books, 3 Opened-Books, 1 Student-Reads, 10.4ms\n",
            "image 38/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0159_jpg.rf.c6811cdfa66e54289e14357027a92a90.jpg: 800x800 6 Opened-Books, 2 Student-Readss, 10.4ms\n",
            "image 39/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0165_jpg.rf.76f1a6b4ea30c2b94448a7431ebfec14.jpg: 800x800 4 Opened-Books, 1 Teacher-Explains, 10.6ms\n",
            "image 40/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0174_jpg.rf.b0f7ce49a25e3a990c3b77cbb2397360.jpg: 800x800 2 Opened-Books, 1 Student-Writes, 10.4ms\n",
            "image 41/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0180_jpg.rf.f48e8236a048b2367561c4f580704347.jpg: 800x800 1 No-Book, 5 Opened-Books, 2 Student-Readss, 1 Worksheet, 10.5ms\n",
            "image 42/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0184_jpg.rf.03b0004776eef1e3a1ebfec87b12a92e.jpg: 800x800 2 Opened-Books, 2 Student-Readss, 10.3ms\n",
            "image 43/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0192_jpg.rf.409e5ffd05ac01552d1c001df0e17d74.jpg: 800x800 3 Opened-Books, 1 Student-Writes, 10.5ms\n",
            "image 44/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0202_jpg.rf.6c9758b1697ec40546d649d149da9542.jpg: 800x800 4 Opened-Books, 2 Student-Readss, 11.2ms\n",
            "image 45/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0204_jpg.rf.601b509d593dac527c12904888ff0cd6.jpg: 800x800 4 Opened-Books, 10.6ms\n",
            "image 46/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0208_jpg.rf.aaaecea1e00c6ade6df1b4a001167408.jpg: 800x800 1 No-Book, 6 Opened-Books, 10.5ms\n",
            "image 47/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0214_jpg.rf.88ab19197ab4bf8ddfc1254c2a7dd08e.jpg: 800x800 1 No-Book, 5 Opened-Books, 10.5ms\n",
            "image 48/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0216_jpg.rf.e2261a6df528b189736fb9554773bc76.jpg: 800x800 1 No-Book, 4 Opened-Books, 1 Student-Reads, 10.5ms\n",
            "image 49/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0236_jpg.rf.baf4caded9731c4d9c78e411400ae8cc.jpg: 800x800 2 Opened-Books, 10.7ms\n",
            "image 50/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0239_jpg.rf.e172ba1fd15a4bb4b1c56bf6a708cfb5.jpg: 800x800 3 Opened-Books, 1 Student-Writes, 10.5ms\n",
            "image 51/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0245_jpg.rf.7ea2ddb292d2c221abb1ac62f54d14e9.jpg: 800x800 1 No-Book, 3 Opened-Books, 10.6ms\n",
            "image 52/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0252_jpg.rf.01d59b9ad0044868645059d56f0063ac.jpg: 800x800 1 No-Book, 1 Student-Answers, 1 Student-Reads, 1 Teacher-Explains, 10.7ms\n",
            "image 53/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0254_jpg.rf.f2a28d00f801a8c24a85f9a74708fb95.jpg: 800x800 1 Student-Answers, 10.5ms\n",
            "image 54/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0262_jpg.rf.360dfe0d6f4f0d60cc9a67ceceadb4a3.jpg: 800x800 1 No-Book, 3 Opened-Books, 10.5ms\n",
            "image 55/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0264_jpg.rf.2bb9f6711b70e8c901133fe97f33d47c.jpg: 800x800 1 No-Book, 6 Opened-Books, 1 Student-Answers, 10.7ms\n",
            "image 56/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_09_04_IMG_1267_MOV-0265_jpg.rf.f6fd1edb83cf01eeda646ed211caa28d.jpg: 800x800 1 No-Book, 6 Opened-Books, 1 Student-Answers, 10.6ms\n",
            "image 57/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0009_jpg.rf.e648aa7d08980be398afe96720db458f.jpg: 800x800 3 Opened-Books, 1 Teacher-Follows-up-Students, 1 Worksheet, 10.6ms\n",
            "image 58/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0046_jpg.rf.545f9f16e8801bc2a31ef3b84a89d5ef.jpg: 800x800 4 Opened-Books, 1 Worksheet, 10.5ms\n",
            "image 59/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0048_jpg.rf.c7d08343bc2039f1f2a3c3266c288268.jpg: 800x800 6 Opened-Books, 2 Worksheets, 10.5ms\n",
            "image 60/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0105_jpg.rf.38facd5a3ad1cc51bf24b4f3f9cb973c.jpg: 800x800 1 No-Book, 5 Opened-Books, 1 Teacher-Explains, 1 Worksheet, 10.5ms\n",
            "image 61/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0115_jpg.rf.a6db3b11b5ec33b8ba36b36aedc9a455.jpg: 800x800 1 Closed-Book, 3 Opened-Books, 3 Worksheets, 10.5ms\n",
            "image 62/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0130_jpg.rf.8f8fc596d26470a179a70a5f7ff87078.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 1 Worksheet, 10.7ms\n",
            "image 63/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0131_jpg.rf.39243eaab155a11fc2995e648e4af0fb.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 1 Worksheet, 10.5ms\n",
            "image 64/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0133_jpg.rf.2709c39368d91bdc287b7b70532ae2df.jpg: 800x800 1 Closed-Book, 10.9ms\n",
            "image 65/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0137_jpg.rf.c8ab59d5e2abb37b5d654eac88f0ade1.jpg: 800x800 1 Opened-Book, 11.5ms\n",
            "image 66/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0145_jpg.rf.64398302febe940456a25aa8aadef9b5.jpg: 800x800 1 Opened-Book, 1 Worksheet, 10.5ms\n",
            "image 67/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0150_jpg.rf.b16f5141141dae71df311787b151b1fb.jpg: 800x800 1 Opened-Book, 2 Worksheets, 10.5ms\n",
            "image 68/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0165_jpg.rf.80cc4dd2ae907ed989bd0f6f5bc4b50e.jpg: 800x800 3 Opened-Books, 10.4ms\n",
            "image 69/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0173_jpg.rf.e147883f984aa5616ce31bda98f21e3d.jpg: 800x800 1 Closed-Book, 10.5ms\n",
            "image 70/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0180_jpg.rf.c4ac5967b1347bde4968c1600a1aec40.jpg: 800x800 1 Worksheet, 10.4ms\n",
            "image 71/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0189_jpg.rf.191c7bb2650ede47beb5de332bb4d45c.jpg: 800x800 1 Opened-Book, 1 Student-Answers, 1 Worksheet, 10.4ms\n",
            "image 72/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0210_jpg.rf.ac6aa51baa7faf39e31393fa10361ccd.jpg: 800x800 1 Opened-Book, 10.5ms\n",
            "image 73/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0212_jpg.rf.7f206240d89524fb825cbd14bb4da8bd.jpg: 800x800 3 Opened-Books, 10.6ms\n",
            "image 74/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0219_jpg.rf.c6dec0fd481f887f8698a90ff1f0007b.jpg: 800x800 (no detections), 11.0ms\n",
            "image 75/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0220_jpg.rf.c819c2a21e6f0b8a51b30f81f75cd322.jpg: 800x800 1 Opened-Book, 10.7ms\n",
            "image 76/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0234_jpg.rf.ed376eda976add3e49ab02537b855d1b.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 1 Student-Answers, 11.4ms\n",
            "image 77/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0237_jpg.rf.920fee005772bc617759085664c6ce10.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 2 Student-Answerss, 11.1ms\n",
            "image 78/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0238_jpg.rf.8ce1f16bca1f7325f9f57f8417aeaf60.jpg: 800x800 2 Closed-Books, 1 Opened-Book, 2 Student-Answerss, 1 Worksheet, 11.7ms\n",
            "image 79/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0251_jpg.rf.2dcd5e58d2372d8a2347751cf7c5d48e.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 4 Student-Answerss, 10.8ms\n",
            "image 80/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0260_jpg.rf.c946f0160e44584d96d60dc5b8a594fc.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 2 Student-Answerss, 1 Worksheet, 11.3ms\n",
            "image 81/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0275_jpg.rf.6ff701595d8a4dd85569f095ab5145a5.jpg: 800x800 3 Opened-Books, 1 Teacher-Explains, 10.6ms\n",
            "image 82/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0277_jpg.rf.d04062458eae2fb70df88cd22a933d41.jpg: 800x800 3 Opened-Books, 10.7ms\n",
            "image 83/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0281_jpg.rf.ee91cbd41dee1709eaca8759a40f6337.jpg: 800x800 2 Worksheets, 10.6ms\n",
            "image 84/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0286_jpg.rf.6527b38bc60f6c6a7c3443649b4bf67c.jpg: 800x800 1 No-Book, 1 Opened-Book, 1 Student-Writes, 10.5ms\n",
            "image 85/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0289_jpg.rf.3222341a796154f10a2da4a72d8e3bb1.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 1 Worksheet, 10.5ms\n",
            "image 86/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0296_jpg.rf.81a19672f0b93e99ad25e2dc67b79132.jpg: 800x800 1 Opened-Book, 1 Worksheet, 10.5ms\n",
            "image 87/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0307_jpg.rf.43dca9fc699697ebdfd5944d447bf0fd.jpg: 800x800 1 Worksheet, 10.5ms\n",
            "image 88/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0323_jpg.rf.d331daa454c292ef41b57bab1f60f636.jpg: 800x800 (no detections), 10.5ms\n",
            "image 89/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0324_jpg.rf.9fc65f61b69a2120753ab8bb84430fed.jpg: 800x800 (no detections), 10.4ms\n",
            "image 90/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0325_jpg.rf.11266ba53469c02959a188f348964ef2.jpg: 800x800 (no detections), 10.5ms\n",
            "image 91/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0327_jpg.rf.8250b27780d267d801aaf7713ca7ea79.jpg: 800x800 (no detections), 10.4ms\n",
            "image 92/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0330_jpg.rf.64859eff4d61e1c69b248a0aa34d4975.jpg: 800x800 (no detections), 11.5ms\n",
            "image 93/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_10_06_IMG_1268_MOV-0339_jpg.rf.86ac130e1ee71e6bbb7aeddb44fa3cf3.jpg: 800x800 (no detections), 10.6ms\n",
            "image 94/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0011_jpg.rf.f9154a70268a96d55b3cb85dd41f1da9.jpg: 800x800 3 Closed-Books, 10.6ms\n",
            "image 95/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0012_jpg.rf.7221a618aea929a69ac71fec56c4c7fd.jpg: 800x800 5 Closed-Books, 1 No-Book, 10.6ms\n",
            "image 96/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0014_jpg.rf.58b1aff2d4cac0d115409d6258e9a113.jpg: 800x800 4 Closed-Books, 1 No-Book, 10.8ms\n",
            "image 97/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0019_jpg.rf.23810b53643fbd17418fd2a46283bbb3.jpg: 800x800 4 Closed-Books, 10.4ms\n",
            "image 98/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0023_jpg.rf.dc89e22aab84225a54f2434495cceef0.jpg: 800x800 6 Closed-Books, 1 No-Book, 10.5ms\n",
            "image 99/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0024_jpg.rf.da26ff33530e3e84b4b64343cce020a4.jpg: 800x800 7 Closed-Books, 1 No-Book, 10.9ms\n",
            "image 100/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0029_jpg.rf.e7b476cdab9eb9980fc1ce62ca3de4dd.jpg: 800x800 3 Closed-Books, 10.3ms\n",
            "image 101/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0045_jpg.rf.cc3756bebc97e246cac4919ab8b5d37f.jpg: 800x800 5 Closed-Books, 1 No-Book, 10.3ms\n",
            "image 102/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0058_jpg.rf.1ad65dc98e78b4c1f21867a3325757a0.jpg: 800x800 5 Closed-Books, 1 No-Book, 10.3ms\n",
            "image 103/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0067_jpg.rf.b1a6ee60ff6546e9f06b04f9ca825f2e.jpg: 800x800 4 Closed-Books, 1 No-Book, 10.3ms\n",
            "image 104/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0070_jpg.rf.d7538056b90dfeb038b8b4f2d86de65f.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 11.2ms\n",
            "image 105/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0130_jpg.rf.a2dc142848d5abb4358da573c1894801.jpg: 800x800 3 Closed-Books, 1 Opened-Book, 1 Raising-Hand, 10.4ms\n",
            "image 106/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0139_jpg.rf.e86e964a51c159011570dfb6bf1682f5.jpg: 800x800 2 Closed-Books, 1 Worksheet, 10.4ms\n",
            "image 107/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0147_jpg.rf.a56981b24b728aff5e85277bbbcf2f78.jpg: 800x800 1 Closed-Book, 1 Worksheet, 10.3ms\n",
            "image 108/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0152_jpg.rf.c9480107971835f2e760c4e146071add.jpg: 800x800 2 Closed-Books, 10.3ms\n",
            "image 109/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0162_jpg.rf.f97ee2f1c442a400cd198c8856d404a0.jpg: 800x800 4 Closed-Books, 1 Opened-Book, 11.8ms\n",
            "image 110/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0169_jpg.rf.d186ae1d9ed9c0d5a8c3c530da819cb0.jpg: 800x800 1 Closed-Book, 10.5ms\n",
            "image 111/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0181_jpg.rf.42b111c0c1277195df1ccf314242115d.jpg: 800x800 2 Closed-Books, 10.4ms\n",
            "image 112/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0192_jpg.rf.72ff79b4d0ae6d0aabb3e32e3d1b6f38.jpg: 800x800 2 Closed-Books, 1 No-Book, 1 Raising-Hand, 10.5ms\n",
            "image 113/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0201_jpg.rf.ec839f3cb7a1a7ebd71eab3e8a365447.jpg: 800x800 2 Closed-Books, 1 Opened-Book, 1 Raising-Hand, 10.5ms\n",
            "image 114/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0204_jpg.rf.1eb0894c328b075f76c7d2ea90c8c7bd.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 1 Raising-Hand, 10.9ms\n",
            "image 115/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0226_jpg.rf.ec40819f041037f41efd29625f022c2c.jpg: 800x800 2 Closed-Books, 1 Teacher-Explains, 10.4ms\n",
            "image 116/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0235_jpg.rf.80f5f655a84d56656f8517e4e632205a.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 11.4ms\n",
            "image 117/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0250_jpg.rf.6ed33120de6ee2a7fff580881587ed9c.jpg: 800x800 2 Closed-Books, 1 Worksheet, 10.9ms\n",
            "image 118/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0264_jpg.rf.da01d8e3a1bc5473b9a0d0d76ce9dceb.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 2 Raising-Hands, 1 Teacher-Explains, 11.1ms\n",
            "image 119/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0287_jpg.rf.4b35b5b1f6e026181c58b51dbbae31a2.jpg: 800x800 2 Closed-Books, 1 Opened-Book, 10.6ms\n",
            "image 120/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0298_jpg.rf.91a072d506250e35e0f1bdda7262f1e9.jpg: 800x800 2 Closed-Books, 1 Opened-Book, 10.4ms\n",
            "image 121/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0316_jpg.rf.4050a765460781b8638f5bbf833ebf8e.jpg: 800x800 4 Closed-Books, 1 Worksheet, 10.3ms\n",
            "image 122/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0321_jpg.rf.200c57f78d1926b80eff6019d9e76ed8.jpg: 800x800 1 Worksheet, 10.6ms\n",
            "image 123/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0340_jpg.rf.0af728450e89267eca3ae645a7c04388.jpg: 800x800 2 Closed-Books, 1 No-Book, 10.4ms\n",
            "image 124/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0349_jpg.rf.10d3dd0dd488e0adf9e0c93992cd48e4.jpg: 800x800 1 Closed-Book, 1 Raising-Hand, 11.1ms\n",
            "image 125/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0356_jpg.rf.18983b6299433880795c4984e7a5c97d.jpg: 800x800 1 Closed-Book, 1 No-Book, 1 Raising-Hand, 11.5ms\n",
            "image 126/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0363_jpg.rf.b017d966c359f49cce74f2f93671fd52.jpg: 800x800 2 Closed-Books, 1 No-Book, 1 Teacher-Follows-up-Students, 11.5ms\n",
            "image 127/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0366_jpg.rf.ba2e61035e7b4512724f7ced17bfef4d.jpg: 800x800 1 Closed-Book, 1 No-Book, 1 Worksheet, 11.6ms\n",
            "image 128/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0374_jpg.rf.37bb52b243b2d0551ef6a57f137dbf31.jpg: 800x800 2 Opened-Books, 2 Student-Readss, 1 Teacher-Explains, 10.8ms\n",
            "image 129/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0379_jpg.rf.1b2444ac771bac0a6a2db90cfe62f481.jpg: 800x800 3 Opened-Books, 1 Student-Reads, 1 Teacher-Explains, 11.2ms\n",
            "image 130/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0380_jpg.rf.7657e1cd509a6e66bcdda539a51bc8e2.jpg: 800x800 (no detections), 10.4ms\n",
            "image 131/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0381_jpg.rf.f8aa197fd9050b194d980873698fac46.jpg: 800x800 2 Opened-Books, 1 Student-Writes, 10.4ms\n",
            "image 132/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0383_jpg.rf.ac8ad6fe51adb3ebae935f0fbbd80811.jpg: 800x800 2 Opened-Books, 10.4ms\n",
            "image 133/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0385_jpg.rf.767e7c3f95d5388b67056963f220508d.jpg: 800x800 3 Opened-Books, 1 Student-Reads, 10.4ms\n",
            "image 134/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0387_jpg.rf.a86a55a30173cee0d77dc4d2c33d6b9e.jpg: 800x800 2 Opened-Books, 10.3ms\n",
            "image 135/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0388_jpg.rf.0ce81576ea295aac22571317fe27dc55.jpg: 800x800 4 Opened-Books, 1 Worksheet, 10.4ms\n",
            "image 136/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0392_jpg.rf.c43f7c240bb4aaf14745c89bc24fc02d.jpg: 800x800 5 Opened-Books, 1 Teacher-Explains, 10.9ms\n",
            "image 137/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0397_jpg.rf.6b4c891b01cbbca6dae471b5194a3da1.jpg: 800x800 4 Opened-Books, 1 Student-Reads, 1 Student-Writes, 1 Worksheet, 12.4ms\n",
            "image 138/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0417_jpg.rf.88cb60caf4d372d7c18b8bb2732eeb94.jpg: 800x800 1 No-Book, 3 Opened-Books, 10.5ms\n",
            "image 139/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0418_jpg.rf.0a7e05dcccf198b6f3802f32d58e8c35.jpg: 800x800 1 No-Book, 3 Opened-Books, 1 Raising-Hand, 11.0ms\n",
            "image 140/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0421_jpg.rf.e2f0c1b972b683e3b0b7b87a533d8be5.jpg: 800x800 3 Opened-Books, 2 Raising-Hands, 1 Teacher-Explains, 10.4ms\n",
            "image 141/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0434_jpg.rf.dadf08ce2fad1e068fb9e3175dd4e044.jpg: 800x800 1 No-Book, 6 Opened-Books, 1 Student-Reads, 10.4ms\n",
            "image 142/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_07_11_35_IMG_1269_MOV-0436_jpg.rf.c80798d477e140be3e6f5e149ad7d0a9.jpg: 800x800 5 Opened-Books, 1 Teacher-Explains, 10.4ms\n",
            "image 143/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0000_jpg.rf.9d2fc4cf25c7284a304be3022688901e.jpg: 800x800 3 Closed-Books, 1 No-Book, 1 Opened-Book, 10.4ms\n",
            "image 144/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0008_jpg.rf.8dee542f653afd0c1fd6bc4431b3e17b.jpg: 800x800 2 Closed-Books, 2 Raising-Hands, 3 Teacher-Explainss, 10.5ms\n",
            "image 145/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0012_jpg.rf.57896fbb29a178ec5e85079a11cb45ca.jpg: 800x800 4 Closed-Books, 2 Raising-Hands, 1 Worksheet, 10.7ms\n",
            "image 146/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0025_jpg.rf.e48cc8b977b757a669c6512d7a28d26c.jpg: 800x800 2 Closed-Books, 1 Teacher-Explains, 11.4ms\n",
            "image 147/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0029_jpg.rf.9b786f4aad4b6cf2a615d35f9b47adf0.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 1 Student-Writes, 1 Worksheet, 11.6ms\n",
            "image 148/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0070_jpg.rf.80cd7c21ed911131ef76989b9328983a.jpg: 800x800 1 Closed-Book, 1 Teacher-Explains, 11.7ms\n",
            "image 149/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0077_jpg.rf.c8fcfb65b2a0a2bf60cd9953ce42dff2.jpg: 800x800 2 Closed-Books, 10.9ms\n",
            "image 150/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0081_jpg.rf.0265a89761bd14b746c773a53b7ff392.jpg: 800x800 (no detections), 11.2ms\n",
            "image 151/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0089_jpg.rf.ceab540752a2c07d148591cfb4026fe1.jpg: 800x800 1 Closed-Book, 1 No-Book, 1 Opened-Book, 1 Student-Reads, 10.8ms\n",
            "image 152/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0106_jpg.rf.b0dd6fa7c437d46b86b9d1dcfff8cb05.jpg: 800x800 2 Closed-Books, 1 No-Book, 1 Teacher-Explains, 13.5ms\n",
            "image 153/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0118_jpg.rf.3d257dbf63d5dada519d5ee1e6ad1abb.jpg: 800x800 2 Closed-Books, 2 No-Books, 2 Opened-Books, 1 Worksheet, 13.5ms\n",
            "image 154/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0120_jpg.rf.7b58c39e09d5ba0af8f4489ed762592c.jpg: 800x800 2 Closed-Books, 1 No-Book, 2 Opened-Books, 1 Teacher-Explains, 10.6ms\n",
            "image 155/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0124_jpg.rf.a38927950416daa049e06f81d13731fd.jpg: 800x800 2 Opened-Books, 12.0ms\n",
            "image 156/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0139_jpg.rf.027d1e233dc9dd6f08370ded1688e787.jpg: 800x800 3 Closed-Books, 3 No-Books, 2 Opened-Books, 1 Teacher-Explains, 10.6ms\n",
            "image 157/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0148_jpg.rf.dad00cb61b22b4b162de5f52da90da5d.jpg: 800x800 3 Closed-Books, 2 Raising-Hands, 10.7ms\n",
            "image 158/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0154_jpg.rf.e8ac5275b06195165a3dabb7d07c8ef1.jpg: 800x800 3 Closed-Books, 3 Opened-Books, 10.5ms\n",
            "image 159/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0159_jpg.rf.ef5d7f06f54c5af67cc936b997d5c403.jpg: 800x800 3 Closed-Books, 1 Opened-Book, 2 Raising-Hands, 10.5ms\n",
            "image 160/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0161_jpg.rf.db1b65299307d092308bc1a4de0a8b21.jpg: 800x800 3 Opened-Books, 1 Raising-Hand, 10.9ms\n",
            "image 161/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0175_jpg.rf.47a8bfd797bbaaaab74c9658653c1529.jpg: 800x800 3 Closed-Books, 3 Opened-Books, 10.6ms\n",
            "image 162/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0201_jpg.rf.b71c999865f68f1e77469c37bab81d84.jpg: 800x800 2 Closed-Books, 1 No-Book, 1 Opened-Book, 11.3ms\n",
            "image 163/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0202_jpg.rf.d5f7b01150191e7b419cc22fc816bffb.jpg: 800x800 2 Closed-Books, 1 No-Book, 1 Opened-Book, 10.5ms\n",
            "image 164/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_07_50_IMG_1271_MOV-0223_jpg.rf.212aae3e97c91bca63e0d81bbc4a13a1.jpg: 800x800 3 Closed-Books, 1 Raising-Hand, 1 Teacher-Explains, 10.4ms\n",
            "image 165/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0002_jpg.rf.d2248eb742e534627829c7f63e12865a.jpg: 800x800 1 Closed-Book, 3 No-Books, 2 Opened-Books, 10.5ms\n",
            "image 166/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0016_jpg.rf.04a020c7333950a410aa671cfa67fd40.jpg: 800x800 2 Closed-Books, 3 No-Books, 1 Opened-Book, 1 Raising-Hand, 1 Teacher-Explains, 10.5ms\n",
            "image 167/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0019_jpg.rf.47a15fada448542241a3718aecc85fc8.jpg: 800x800 2 Closed-Books, 12.5ms\n",
            "image 168/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0036_jpg.rf.673cc64d12ff356860a4e7fc740b0b02.jpg: 800x800 3 No-Books, 1 Opened-Book, 2 Raising-Hands, 1 Worksheet, 10.8ms\n",
            "image 169/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0046_jpg.rf.2ab234893870b0132e4b7f5b36470451.jpg: 800x800 1 Closed-Book, 3 No-Books, 1 Opened-Book, 2 Raising-Hands, 1 Teacher-Explains, 10.6ms\n",
            "image 170/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0049_jpg.rf.9eff3a54915b8570f3467c9bcf5b4c15.jpg: 800x800 4 No-Books, 2 Opened-Books, 5 Raising-Hands, 10.5ms\n",
            "image 171/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0052_jpg.rf.c76e87e6d6e7ea0f6ce88831c98bc6df.jpg: 800x800 2 Closed-Books, 1 No-Book, 1 Teacher-Explains, 10.4ms\n",
            "image 172/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0070_jpg.rf.981a54b3dfd8e9192f441eca157630d6.jpg: 800x800 1 Closed-Book, 1 No-Book, 1 Teacher-Explains, 10.4ms\n",
            "image 173/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0072_jpg.rf.6d8bb816f2ed51a518c0bca7787c2276.jpg: 800x800 1 Closed-Book, 1 Teacher-Explains, 10.7ms\n",
            "image 174/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0073_jpg.rf.6111b31d927c86576b56cd7a354fa1ec.jpg: 800x800 1 Closed-Book, 11.0ms\n",
            "image 175/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0083_jpg.rf.8f4032a1d506b06c047892933562f146.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 10.9ms\n",
            "image 176/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0091_jpg.rf.d13502c06c15b003d7b71ad7e06ae5b5.jpg: 800x800 1 Closed-Book, 3 No-Books, 1 Opened-Book, 1 Raising-Hand, 1 Teacher-Explains, 10.7ms\n",
            "image 177/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0094_jpg.rf.394f1cf152e697b746cd67b7fb287eb0.jpg: 800x800 3 No-Books, 2 Opened-Books, 1 Raising-Hand, 1 Teacher-Explains, 10.7ms\n",
            "image 178/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0122_jpg.rf.ea588ff52c3618240651b18685cf3c79.jpg: 800x800 1 Closed-Book, 10.7ms\n",
            "image 179/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0139_jpg.rf.0a34d657dc2237036336fe1340d365c4.jpg: 800x800 1 Closed-Book, 10.8ms\n",
            "image 180/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0149_jpg.rf.5d59328eccd7eb5e4a51eab0e26f0efe.jpg: 800x800 1 Opened-Book, 1 Worksheet, 10.7ms\n",
            "image 181/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0183_jpg.rf.0849ecccb076827823200e74587aa96f.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 11.6ms\n",
            "image 182/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0192_jpg.rf.74bdda234afca27ae2e9db548b742e27.jpg: 800x800 4 Closed-Books, 1 Opened-Book, 12.5ms\n",
            "image 183/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0193_jpg.rf.9b85496f568346e915fab3e7bb352fb3.jpg: 800x800 4 Closed-Books, 1 No-Book, 1 Opened-Book, 12.2ms\n",
            "image 184/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0196_jpg.rf.339c9bd0cfae7deccc05a140dbb26812.jpg: 800x800 3 Closed-Books, 3 No-Books, 1 Opened-Book, 3 Raising-Hands, 10.6ms\n",
            "image 185/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0210_jpg.rf.fc709df78d0e2c5621832dda3b2c7b2c.jpg: 800x800 1 Student-Answers, 10.5ms\n",
            "image 186/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0212_jpg.rf.a88981197f9558d83b846f9df53ab07a.jpg: 800x800 1 Student-Answers, 10.5ms\n",
            "image 187/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0213_jpg.rf.c888b62067400c1746de94d8e65dde99.jpg: 800x800 1 Student-Answers, 10.4ms\n",
            "image 188/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_08_09_30_IMG_1272_MOV-0216_jpg.rf.55eca291670ce2959ebda1d95b0f45b3.jpg: 800x800 1 Student-Answers, 10.5ms\n",
            "image 189/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0006_jpg.rf.5a4b0f5f7ee80b69da5adb95a4256174.jpg: 800x800 3 No-Books, 1 Raising-Hand, 10.5ms\n",
            "image 190/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0007_jpg.rf.479e9d2b0d32f1bce0db21d1641bd034.jpg: 800x800 3 No-Books, 1 Raising-Hand, 1 Teacher-Explains, 10.6ms\n",
            "image 191/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0010_jpg.rf.458334bae8d26fcf13d7a90b5de6b70a.jpg: 800x800 3 No-Books, 3 Raising-Hands, 10.8ms\n",
            "image 192/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0020_jpg.rf.570afbee38bb22fc90b824a1e2986c97.jpg: 800x800 3 No-Books, 3 Raising-Hands, 10.6ms\n",
            "image 193/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0022_jpg.rf.dd3043bc0b2dfa424d01302a0e37758b.jpg: 800x800 3 No-Books, 4 Raising-Hands, 1 Teacher-Explains, 11.1ms\n",
            "image 194/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0025_jpg.rf.a2fba6d64b5a58ad2ed055fefa517024.jpg: 800x800 4 No-Books, 1 Worksheet, 10.6ms\n",
            "image 195/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0026_jpg.rf.2ca78250ba6cd516c2065d704daa661a.jpg: 800x800 3 No-Books, 1 Worksheet, 10.6ms\n",
            "image 196/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0047_jpg.rf.02bedf3ed7927807b217efe1b49c796f.jpg: 800x800 3 No-Books, 10.6ms\n",
            "image 197/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0054_jpg.rf.ebc81b1977a8f4d347425008e98ab282.jpg: 800x800 4 No-Books, 1 Raising-Hand, 10.5ms\n",
            "image 198/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0058_jpg.rf.1ad771654fdd80827e1e26c427a4ee2f.jpg: 800x800 2 No-Books, 4 Raising-Hands, 10.5ms\n",
            "image 199/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0069_jpg.rf.f550414bedd19c43f1bc1c67f5cd7ba5.jpg: 800x800 4 No-Books, 10.4ms\n",
            "image 200/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0077_jpg.rf.99476b04dae29797bf163335eded3154.jpg: 800x800 4 No-Books, 2 Raising-Hands, 10.5ms\n",
            "image 201/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0101_jpg.rf.c41944b7b69a75c3d455b0d117cb2a2e.jpg: 800x800 3 No-Books, 2 Raising-Hands, 10.5ms\n",
            "image 202/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0103_jpg.rf.bb6e52b977ee36e33673c97882edc767.jpg: 800x800 3 No-Books, 5 Raising-Hands, 10.5ms\n",
            "image 203/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0111_jpg.rf.e5c3f78ef77cd16174125ec80047e8c6.jpg: 800x800 4 No-Books, 1 Raising-Hand, 10.5ms\n",
            "image 204/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0119_jpg.rf.7731e4402595ac03af15ac1757bb575a.jpg: 800x800 4 No-Books, 10.5ms\n",
            "image 205/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0123_jpg.rf.b0ea6664fc8febcf1a27719c94dd83c2.jpg: 800x800 3 No-Books, 13.2ms\n",
            "image 206/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0128_jpg.rf.e72423706b49e080da72856c3ec1c286.jpg: 800x800 4 No-Books, 1 Teacher-Explains, 10.5ms\n",
            "image 207/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0136_jpg.rf.f244c81c7e0a2718b0673696b153fe12.jpg: 800x800 1 No-Book, 4 Raising-Hands, 1 Teacher-Explains, 10.5ms\n",
            "image 208/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0143_jpg.rf.08fd83b0fe5b6585ed2ba1b3a225326b.jpg: 800x800 3 No-Books, 3 Raising-Hands, 1 Teacher-Explains, 10.9ms\n",
            "image 209/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0149_jpg.rf.a71bf2cd3ac0df7535745b64257cfeb3.jpg: 800x800 2 No-Books, 1 Teacher-Explains, 10.5ms\n",
            "image 210/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0152_jpg.rf.7315a61f3f41f5b6558bf5c740b0f8e5.jpg: 800x800 4 No-Books, 10.6ms\n",
            "image 211/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0153_jpg.rf.bde7917bd7a638292628a8f03d6ac606.jpg: 800x800 4 No-Books, 10.7ms\n",
            "image 212/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0161_jpg.rf.01681a4757a469aed6b3334d433615ac.jpg: 800x800 4 No-Books, 1 Raising-Hand, 10.8ms\n",
            "image 213/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0169_jpg.rf.2f4b9173e2b65fe1a5ad370970f12596.jpg: 800x800 1 Student-Answers, 10.6ms\n",
            "image 214/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0175_jpg.rf.24424a9a2f36306a2f78f1c9224423bb.jpg: 800x800 1 Student-Answers, 10.5ms\n",
            "image 215/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0177_jpg.rf.c65679766a1ab4e38f9d00f33e1305c6.jpg: 800x800 4 No-Books, 3 Raising-Hands, 10.5ms\n",
            "image 216/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0180_jpg.rf.04ffb63511cd91721fecbc1015552b63.jpg: 800x800 1 Student-Answers, 10.5ms\n",
            "image 217/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0188_jpg.rf.ee3e707dfe468ec1eb23cef1b4bbca13.jpg: 800x800 3 No-Books, 1 Worksheet, 10.5ms\n",
            "image 218/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0210_jpg.rf.14003fd70bd51671e6055ee823976925.jpg: 800x800 4 No-Books, 1 Worksheet, 10.5ms\n",
            "image 219/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0214_jpg.rf.cf765970b089d3c51946b116cddbcc18.jpg: 800x800 2 No-Books, 1 Raising-Hand, 1 Worksheet, 10.4ms\n",
            "image 220/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0223_jpg.rf.b3686623ecb0a2f8c9718df2fd6861de.jpg: 800x800 3 No-Books, 1 Raising-Hand, 1 Worksheet, 10.5ms\n",
            "image 221/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0236_jpg.rf.4fffebe8b9dc2249bf9b57bfeb53e320.jpg: 800x800 3 No-Books, 3 Raising-Hands, 1 Worksheet, 10.5ms\n",
            "image 222/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0251_jpg.rf.b7932e2c03e5bd7cc8caee1deb18dfca.jpg: 800x800 5 No-Books, 2 Raising-Hands, 1 Worksheet, 10.7ms\n",
            "image 223/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_08_55_IMG_1317_MOV-0275_jpg.rf.b1db8353ef7cba3eaf6b64ddd1a426d0.jpg: 800x800 2 No-Books, 1 Worksheet, 10.7ms\n",
            "image 224/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0038_jpg.rf.225258d880be8f31bad56d63e7023b49.jpg: 800x800 4 Opened-Books, 1 Student-Answers, 10.7ms\n",
            "image 225/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0042_jpg.rf.3bc7f11f22384e2981258764486f9dd3.jpg: 800x800 3 Opened-Books, 11.5ms\n",
            "image 226/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0047_jpg.rf.5e630fe9d9a30c48c616e488efbcf727.jpg: 800x800 2 Opened-Books, 11.0ms\n",
            "image 227/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0053_jpg.rf.6660acdaba54dfb83a0952ca467fa712.jpg: 800x800 1 No-Book, 2 Opened-Books, 11.7ms\n",
            "image 228/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0060_jpg.rf.3b35430faa44a371569fa65b77c593d9.jpg: 800x800 2 No-Books, 2 Opened-Books, 10.7ms\n",
            "image 229/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0061_jpg.rf.afff37fdb7cd8e24c9cb62b90543a869.jpg: 800x800 2 No-Books, 2 Opened-Books, 10.6ms\n",
            "image 230/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0084_jpg.rf.07fbd905606bbac9b570010976c68719.jpg: 800x800 5 Opened-Books, 10.6ms\n",
            "image 231/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0092_jpg.rf.82794b228663a3bf90fce319a7b8c8bc.jpg: 800x800 1 No-Book, 2 Opened-Books, 11.2ms\n",
            "image 232/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0100_jpg.rf.89f97a243d46edd71cc9b3977f33026c.jpg: 800x800 1 No-Book, 4 Opened-Books, 10.7ms\n",
            "image 233/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0101_jpg.rf.c101aa52a69ba0758d314e8f81c020be.jpg: 800x800 5 Opened-Books, 10.6ms\n",
            "image 234/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0107_jpg.rf.b50958c0a22320e53f0518da46baccd2.jpg: 800x800 1 No-Book, 3 Opened-Books, 10.6ms\n",
            "image 235/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0108_jpg.rf.de884f9075f7c96a69863d4f5eec4df1.jpg: 800x800 5 Opened-Books, 2 Student-Writess, 10.5ms\n",
            "image 236/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0110_jpg.rf.2c8a781a217776093c70fb6bd07e176c.jpg: 800x800 1 No-Book, 5 Opened-Books, 10.5ms\n",
            "image 237/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0113_jpg.rf.911483b48c519fe4145c8572e8f5683b.jpg: 800x800 1 No-Book, 3 Opened-Books, 10.6ms\n",
            "image 238/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0123_jpg.rf.876ea4d0f6b80c945543c90ee1532de1.jpg: 800x800 2 No-Books, 3 Opened-Books, 10.8ms\n",
            "image 239/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0128_jpg.rf.f94ea7fb22ec89f54f216b754ef2c373.jpg: 800x800 1 No-Book, 4 Opened-Books, 11.7ms\n",
            "image 240/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0148_jpg.rf.d4c87dfbfd1e8c20c2dd26c4921b928c.jpg: 800x800 2 No-Books, 3 Opened-Books, 10.6ms\n",
            "image 241/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0152_jpg.rf.069e0f9976e09bb67f91d4b4d1d47733.jpg: 800x800 3 Opened-Books, 1 Student-Writes, 10.5ms\n",
            "image 242/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0158_jpg.rf.442d192281d94bd9d63193c7a27efcb5.jpg: 800x800 1 No-Book, 1 Opened-Book, 1 Student-Reads, 10.5ms\n",
            "image 243/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0173_jpg.rf.5c7b9b4e66de6e0b199e8040a9f6b099.jpg: 800x800 2 No-Books, 1 Opened-Book, 1 Student-Writes, 10.7ms\n",
            "image 244/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0180_jpg.rf.63a54134f07e7119c1425f559a442e82.jpg: 800x800 4 Opened-Books, 10.6ms\n",
            "image 245/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0185_jpg.rf.002a51c1bfb88832214f142b93a156c7.jpg: 800x800 2 No-Books, 3 Opened-Books, 1 Raising-Hand, 10.6ms\n",
            "image 246/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0194_jpg.rf.6025c8a329900e8572bd4b33fb1688fe.jpg: 800x800 2 No-Books, 4 Opened-Books, 10.8ms\n",
            "image 247/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0209_jpg.rf.346e49c992ee902c8e5b48c7cd329e3b.jpg: 800x800 2 Opened-Books, 1 Student-Writes, 10.7ms\n",
            "image 248/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0216_jpg.rf.5f958cd2ac49fc059680ee5f6226a235.jpg: 800x800 2 Opened-Books, 10.6ms\n",
            "image 249/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0220_jpg.rf.746e10a13f28f0d208bbdb5f63831866.jpg: 800x800 6 Opened-Books, 1 Student-Reads, 2 Student-Writess, 10.5ms\n",
            "image 250/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0223_jpg.rf.3f781edf541112959ac1f504d506bad5.jpg: 800x800 1 No-Book, 3 Opened-Books, 2 Student-Writess, 10.5ms\n",
            "image 251/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0227_jpg.rf.620ae63613409752ac884a21e5ce7939.jpg: 800x800 (no detections), 10.9ms\n",
            "image 252/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0244_jpg.rf.da06c70539bacdee5d81bb07e01edc65.jpg: 800x800 1 No-Book, 11.0ms\n",
            "image 253/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0269_jpg.rf.32d2d14fba91bcd13624f9baab04bd58.jpg: 800x800 2 No-Books, 2 Opened-Books, 10.7ms\n",
            "image 254/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0271_jpg.rf.ad542d5f47fa95efe5b8fa5c70f3c4f0.jpg: 800x800 4 Opened-Books, 11.7ms\n",
            "image 255/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0272_jpg.rf.ef313efc6d54e7e4ced6fb8fa09cd192.jpg: 800x800 3 Opened-Books, 1 Student-Writes, 11.0ms\n",
            "image 256/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0279_jpg.rf.34f691783262df70def6906b1de13724.jpg: 800x800 (no detections), 10.7ms\n",
            "image 257/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0285_jpg.rf.c623817068fc71f3e8300f468f003422.jpg: 800x800 1 No-Book, 1 Opened-Book, 1 Student-Writes, 10.6ms\n",
            "image 258/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0295_jpg.rf.b52c575430b34427a6ec9dd7f74fa8c9.jpg: 800x800 1 No-Book, 4 Opened-Books, 1 Student-Writes, 10.5ms\n",
            "image 259/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_09_26_IMG_1318-001_MOV-0329_jpg.rf.8a786778a045ba667ca4d924179ec0eb.jpg: 800x800 3 Opened-Books, 10.4ms\n",
            "image 260/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0000_jpg.rf.21a598428e312ef368029caae3995ec9.jpg: 800x800 1 No-Book, 2 Opened-Books, 1 Student-Reads, 10.5ms\n",
            "image 261/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0002_jpg.rf.44fd0ed36371b6b7221fccdddb35a5e5.jpg: 800x800 3 Opened-Books, 10.7ms\n",
            "image 262/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0011_jpg.rf.b3cb2e83bff7d551bb9073fd9d12afbf.jpg: 800x800 1 Closed-Book, 1 No-Book, 1 Opened-Book, 10.9ms\n",
            "image 263/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0022_jpg.rf.26ed577bf7fd3e3670b6889d34887e6b.jpg: 800x800 1 Closed-Book, 1 No-Book, 2 Opened-Books, 10.9ms\n",
            "image 264/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0023_jpg.rf.98c0528e8729a968003f717d2756c05b.jpg: 800x800 1 No-Book, 3 Opened-Books, 1 Raising-Hand, 1 Student-Writes, 10.8ms\n",
            "image 265/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0024_jpg.rf.e0998e757ae51a70b47194a0eefd24ce.jpg: 800x800 1 No-Book, 1 Opened-Book, 2 Raising-Hands, 10.8ms\n",
            "image 266/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0027_jpg.rf.90b784012c43d754897eb05691c0742d.jpg: 800x800 1 No-Book, 1 Opened-Book, 1 Raising-Hand, 10.7ms\n",
            "image 267/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0028_jpg.rf.69e4066a3214b9df7afff350c29a951b.jpg: 800x800 1 No-Book, 2 Opened-Books, 1 Raising-Hand, 10.8ms\n",
            "image 268/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0034_jpg.rf.52cc20d5ab405cb6a031de34bad74620.jpg: 800x800 2 Opened-Books, 10.7ms\n",
            "image 269/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0036_jpg.rf.fef977edf638bc0e5895f9ef935da1fc.jpg: 800x800 1 No-Book, 2 Opened-Books, 1 Student-Reads, 10.8ms\n",
            "image 270/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0039_jpg.rf.653c42f2e467fb455224ba1b3a28f46f.jpg: 800x800 1 Closed-Book, 1 No-Book, 2 Opened-Books, 4 Raising-Hands, 11.0ms\n",
            "image 271/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0049_jpg.rf.c89b6975d48ce1e349e25d4b8c3429cf.jpg: 800x800 1 No-Book, 3 Opened-Books, 10.8ms\n",
            "image 272/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0051_jpg.rf.8d3f99b7e50de877ed49601f0e530a0e.jpg: 800x800 1 No-Book, 2 Opened-Books, 10.7ms\n",
            "image 273/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0059_jpg.rf.c583aae515c952f1e3fd07855286cdb0.jpg: 800x800 1 Opened-Book, 1 Raising-Hand, 10.7ms\n",
            "image 274/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0065_jpg.rf.aad815f7562dd97ad900bd11ded82169.jpg: 800x800 1 No-Book, 3 Opened-Books, 10.8ms\n",
            "image 275/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0078_jpg.rf.747890b5e9698d6646dd377a9fe83852.jpg: 800x800 2 Opened-Books, 10.6ms\n",
            "image 276/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0102_jpg.rf.a4ae6d0598c97bf4c6bdf24155c58248.jpg: 800x800 1 Closed-Book, 1 No-Book, 2 Opened-Books, 10.7ms\n",
            "image 277/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0125_jpg.rf.b5a3b338467764a6aca3df61600ace38.jpg: 800x800 1 Closed-Book, 1 No-Book, 2 Opened-Books, 1 Student-Reads, 10.6ms\n",
            "image 278/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0129_jpg.rf.525d51d68bd1c1dc8b6c341fc0f25f3f.jpg: 800x800 1 No-Book, 1 Opened-Book, 10.6ms\n",
            "image 279/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0135_jpg.rf.9711ecea64cc1bf7dc187a49d6e4a44d.jpg: 800x800 1 Closed-Book, 4 Opened-Books, 1 Student-Reads, 10.5ms\n",
            "image 280/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0140_jpg.rf.0afed61e2596d7df5b7f380b47759f71.jpg: 800x800 1 Closed-Book, 1 No-Book, 4 Opened-Books, 1 Raising-Hand, 10.5ms\n",
            "image 281/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0145_jpg.rf.d7ee61b2ed71a56e9824f666889512cb.jpg: 800x800 2 Closed-Books, 2 No-Books, 3 Opened-Books, 1 Raising-Hand, 10.5ms\n",
            "image 282/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0157_jpg.rf.4b478d45fb23fa157a4a4906218508c6.jpg: 800x800 3 Closed-Books, 2 No-Books, 1 Opened-Book, 1 Raising-Hand, 10.6ms\n",
            "image 283/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0170_jpg.rf.ce0c58e9df65c339a9f255a9585e0430.jpg: 800x800 1 No-Book, 3 Opened-Books, 1 Raising-Hand, 1 Teacher-Explains, 10.6ms\n",
            "image 284/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0176_jpg.rf.cb51e5dfd8e355de190efb74fea7cf37.jpg: 800x800 2 Closed-Books, 2 No-Books, 2 Opened-Books, 2 Raising-Hands, 1 Student-Reads, 10.8ms\n",
            "image 285/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0190_jpg.rf.08d68babe6008160a0568c687f2587f2.jpg: 800x800 1 Closed-Book, 2 No-Books, 2 Opened-Books, 1 Raising-Hand, 10.6ms\n",
            "image 286/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0192_jpg.rf.3e688689b819903d6356d46e5d9451f3.jpg: 800x800 1 Closed-Book, 2 No-Books, 2 Opened-Books, 10.5ms\n",
            "image 287/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0194_jpg.rf.d9a6b0fcd083c3cff50e7e74ac1c89d9.jpg: 800x800 1 Closed-Book, 2 No-Books, 3 Opened-Books, 10.4ms\n",
            "image 288/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0210_jpg.rf.976a87bc728f9c374e638eb9e34d0c92.jpg: 800x800 3 Opened-Books, 1 Student-Reads, 10.4ms\n",
            "image 289/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0224_jpg.rf.c77f1cbf5fb8ae17edb6c883bdaf0163.jpg: 800x800 1 Closed-Book, 1 No-Book, 1 Opened-Book, 1 Student-Writes, 10.4ms\n",
            "image 290/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0233_jpg.rf.c3393ed0f7e689b9d173982ec1dce79f.jpg: 800x800 1 Closed-Book, 2 No-Books, 1 Opened-Book, 10.5ms\n",
            "image 291/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0235_jpg.rf.5f38bb9e0b44e86388051d626b0ed043.jpg: 800x800 1 Closed-Book, 1 No-Book, 2 Opened-Books, 10.5ms\n",
            "image 292/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0239_jpg.rf.29fd1c98125a941ba60b653afa284524.jpg: 800x800 4 Opened-Books, 2 Student-Writess, 10.6ms\n",
            "image 293/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0241_jpg.rf.dadd2c7388e031a8b2d897ce66b18087.jpg: 800x800 3 Opened-Books, 1 Student-Reads, 1 Student-Writes, 10.6ms\n",
            "image 294/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0253_jpg.rf.fac545c3d33aefef240eee0a26771805.jpg: 800x800 2 Closed-Books, 1 No-Book, 1 Opened-Book, 1 Student-Reads, 1 Worksheet, 10.5ms\n",
            "image 295/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0294_jpg.rf.316593c6230c4c14c835575a95b51a14.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 1 Student-Reads, 10.5ms\n",
            "image 296/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0300_jpg.rf.7d0960e132ea0ca12b9fb50f44d0cd0e.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 1 Student-Reads, 10.5ms\n",
            "image 297/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0313_jpg.rf.6110bfd98f81dbc9b9a94f536bd1338b.jpg: 800x800 1 Closed-Book, 2 No-Books, 4 Opened-Books, 2 Raising-Hands, 1 Student-Reads, 11.7ms\n",
            "image 298/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0314_jpg.rf.565f1071a1ee3f07abc4359165354c57.jpg: 800x800 2 Closed-Books, 2 No-Books, 3 Opened-Books, 1 Raising-Hand, 1 Student-Reads, 10.6ms\n",
            "image 299/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0321_jpg.rf.433e0173d8bfa809d79f6814ce2d085b.jpg: 800x800 1 Closed-Book, 1 No-Book, 10.5ms\n",
            "image 300/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0326_jpg.rf.12c0e37f8c494a24f4120bc33a9531c8.jpg: 800x800 1 Closed-Book, 2 No-Books, 3 Opened-Books, 1 Student-Reads, 10.8ms\n",
            "image 301/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_21_10_06_IMG_1319-005_MOV-0338_jpg.rf.af979ff711a5eb0754d8d168197c8b9f.jpg: 800x800 1 Closed-Book, 1 No-Book, 1 Opened-Book, 1 Student-Reads, 10.6ms\n",
            "image 302/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_08_IMG_1321-003_MOV-0000_jpg.rf.83ea040901e4b4723ea6627edf0183e8.jpg: 800x800 1 No-Book, 5 Opened-Books, 10.4ms\n",
            "image 303/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_08_IMG_1321-003_MOV-0015_jpg.rf.a4cc581ac1ff1e9245e9048291a60073.jpg: 800x800 5 Opened-Books, 10.5ms\n",
            "image 304/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_08_IMG_1321-003_MOV-0017_jpg.rf.bb30c00b569c3948e7df0fc8ab384c53.jpg: 800x800 2 Opened-Books, 10.5ms\n",
            "image 305/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_08_IMG_1321-003_MOV-0034_jpg.rf.c11aee41910181c40408ed6d7d65a874.jpg: 800x800 1 No-Book, 2 Opened-Books, 10.6ms\n",
            "image 306/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_08_IMG_1321-003_MOV-0062_jpg.rf.093ba94a0b6463300fdf189f5e6ba21f.jpg: 800x800 1 No-Book, 2 Opened-Books, 10.7ms\n",
            "image 307/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_08_IMG_1321-003_MOV-0066_jpg.rf.9872f96ccb587fcbd726801a8c35dbc5.jpg: 800x800 1 No-Book, 1 Opened-Book, 1 Student-Reads, 11.0ms\n",
            "image 308/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_08_IMG_1321-003_MOV-0076_jpg.rf.fd9b3370604921021f9e228eaaf17278.jpg: 800x800 1 No-Book, 2 Opened-Books, 10.8ms\n",
            "image 309/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_08_IMG_1321-003_MOV-0091_jpg.rf.11eee9d16d024f4b92e0dd6a1c147b0b.jpg: 800x800 2 No-Books, 5 Opened-Books, 11.7ms\n",
            "image 310/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_08_IMG_1321-003_MOV-0095_jpg.rf.0e43335fd7bfc69efe3b8405268070de.jpg: 800x800 1 No-Book, 5 Opened-Books, 4 Raising-Hands, 11.4ms\n",
            "image 311/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_08_IMG_1321-003_MOV-0099_jpg.rf.8135683800fbb019baf73d3dfa9004f5.jpg: 800x800 3 Opened-Books, 2 Raising-Hands, 10.8ms\n",
            "image 312/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_08_IMG_1321-003_MOV-0103_jpg.rf.db69ad1b09544dc5ac351d2af9d8ee6a.jpg: 800x800 1 No-Book, 4 Opened-Books, 10.8ms\n",
            "image 313/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_08_IMG_1321-003_MOV-0128_jpg.rf.6d836b75da38cc0d73955bce74a6b9ec.jpg: 800x800 1 No-Book, 2 Opened-Books, 1 Student-Reads, 10.6ms\n",
            "image 314/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_08_IMG_1321-003_MOV-0135_jpg.rf.2fe37b4a05106f1cc7980ad3e1fdbd9f.jpg: 800x800 3 Opened-Books, 10.6ms\n",
            "image 315/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_08_IMG_1321-003_MOV-0151_jpg.rf.cbaaf1baae7ddf45ceefd814918d46b0.jpg: 800x800 3 Opened-Books, 2 Student-Writess, 10.7ms\n",
            "image 316/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_08_IMG_1321-003_MOV-0155_jpg.rf.36ae947324e4be7f0cf7c724e73722e4.jpg: 800x800 1 No-Book, 1 Opened-Book, 1 Student-Answers, 10.6ms\n",
            "image 317/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_08_IMG_1321-003_MOV-0202_jpg.rf.ae193b1052a044e730b1902b7315978a.jpg: 800x800 2 Opened-Books, 1 Raising-Hand, 10.6ms\n",
            "image 318/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_08_IMG_1321-003_MOV-0222_jpg.rf.866d39b8b05faf5aa0858ec6fa6bac37.jpg: 800x800 5 Opened-Books, 10.5ms\n",
            "image 319/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0001_jpg.rf.7c42dd722f7c72b6a1e0114ef4d95919.jpg: 800x800 1 Closed-Book, 3 No-Books, 2 Opened-Books, 10.6ms\n",
            "image 320/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0005_jpg.rf.a733b165bef25f2cb7bef5586c982eda.jpg: 800x800 3 No-Books, 2 Opened-Books, 10.7ms\n",
            "image 321/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0012_jpg.rf.a53a3653236e09740131be946d959998.jpg: 800x800 3 Opened-Books, 1 Raising-Hand, 10.8ms\n",
            "image 322/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0013_jpg.rf.31acf8ff32a6fa53cb32986065a799d3.jpg: 800x800 1 No-Book, 3 Opened-Books, 1 Raising-Hand, 1 Student-Reads, 10.7ms\n",
            "image 323/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0023_jpg.rf.d3c3f3db290d212ea29dfb1b9d6f9f56.jpg: 800x800 1 Closed-Book, 1 No-Book, 2 Opened-Books, 1 Raising-Hand, 10.7ms\n",
            "image 324/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0027_jpg.rf.cdadb6ca6c01fd7f9444c30c9bfbe922.jpg: 800x800 2 Closed-Books, 1 No-Book, 1 Opened-Book, 1 Raising-Hand, 10.6ms\n",
            "image 325/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0070_jpg.rf.e1ff2c3a8e5cd99228662eed9adebdd1.jpg: 800x800 2 Closed-Books, 1 No-Book, 1 Opened-Book, 1 Raising-Hand, 10.6ms\n",
            "image 326/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0093_jpg.rf.4a0edebba15960c0306bbd88c4d4c6d9.jpg: 800x800 2 Closed-Books, 1 No-Book, 1 Opened-Book, 2 Raising-Hands, 1 Student-Answers, 10.6ms\n",
            "image 327/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0117_jpg.rf.990617448f96c4b20269c04d4be4ec91.jpg: 800x800 2 Closed-Books, 1 No-Book, 1 Opened-Book, 1 Raising-Hand, 10.6ms\n",
            "image 328/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0118_jpg.rf.d811713755834ab5ed06d5baca57ed7b.jpg: 800x800 2 Closed-Books, 1 No-Book, 1 Opened-Book, 11.1ms\n",
            "image 329/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0120_jpg.rf.21c02210c68a53a3d97e14c2de26c05a.jpg: 800x800 2 Closed-Books, 1 Opened-Book, 1 Raising-Hand, 10.8ms\n",
            "image 330/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0159_jpg.rf.a795d6c378b21249023cb070c191fed8.jpg: 800x800 1 Closed-Book, 2 Opened-Books, 1 Student-Answers, 10.7ms\n",
            "image 331/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0168_jpg.rf.7f6e15a1fee9b0de0242f34f9fb564c7.jpg: 800x800 1 Closed-Book, 1 No-Book, 10.7ms\n",
            "image 332/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0177_jpg.rf.a3b6bd9d552604f0204dfb10e27c31e4.jpg: 800x800 1 Closed-Book, 2 Opened-Books, 11.0ms\n",
            "image 333/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0178_jpg.rf.7526b05eb2ba53a26f7ea22f1da34fb4.jpg: 800x800 1 Closed-Book, 1 No-Book, 1 Opened-Book, 10.6ms\n",
            "image 334/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0183_jpg.rf.acc1372ebf8b1f2bc9f8b720be2f922b.jpg: 800x800 1 Closed-Book, 1 No-Book, 1 Opened-Book, 10.6ms\n",
            "image 335/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0190_jpg.rf.7140f950adf096b3becebd2a7a076475.jpg: 800x800 2 Closed-Books, 10.6ms\n",
            "image 336/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0191_jpg.rf.b23f36467e2a0183c7e553f8359de24e.jpg: 800x800 2 Closed-Books, 2 No-Books, 11.2ms\n",
            "image 337/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0211_jpg.rf.2c6f7fc2486baf2ef679c0b7de278980.jpg: 800x800 2 Closed-Books, 2 Raising-Hands, 10.7ms\n",
            "image 338/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0217_jpg.rf.45985bce13e562c29ac43d1d92e5f162.jpg: 800x800 3 Closed-Books, 1 Opened-Book, 10.6ms\n",
            "image 339/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0221_jpg.rf.f962e4f07709d1b2cd1961ebfc19d8e7.jpg: 800x800 3 Closed-Books, 1 No-Book, 1 Opened-Book, 10.6ms\n",
            "image 340/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0242_jpg.rf.7cd624a40432e16cad2b0fc584bb4958.jpg: 800x800 3 Closed-Books, 1 Opened-Book, 5 Raising-Hands, 10.6ms\n",
            "image 341/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0272_jpg.rf.460c71ac4ad5ccc08801a8d943497759.jpg: 800x800 2 Closed-Books, 1 Opened-Book, 10.6ms\n",
            "image 342/452 /content/datasets/students-behaviors-detection-4/test/images/2024_05_22_08_30_IMG_1322-002_MOV-0291_jpg.rf.5021cfb6d563c2cb82d6d8d95e86c8c1.jpg: 800x800 3 Closed-Books, 2 No-Books, 10.7ms\n",
            "image 343/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0006_jpg.rf.86eecae0a6bb0e8478390fdbdd5d8cb8.jpg: 800x800 1 Closed-Book, 3 Worksheets, 10.7ms\n",
            "image 344/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0007_jpg.rf.ec051251a54f1d2e88bb3cc76fea2a81.jpg: 800x800 4 Closed-Books, 1 No-Book, 1 Student-Answers, 10.6ms\n",
            "image 345/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0010_jpg.rf.8d01f4fc726bcac9c13bad711433db60.jpg: 800x800 4 Closed-Books, 1 No-Book, 1 Student-Answers, 10.8ms\n",
            "image 346/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0016_jpg.rf.6ff7c5c2775130588c34a3e5b49b3d0d.jpg: 800x800 4 Closed-Books, 1 No-Book, 10.7ms\n",
            "image 347/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0018_jpg.rf.03aea1b1240085d185fb382f3a7d7e8c.jpg: 800x800 1 Closed-Book, 2 Worksheets, 10.6ms\n",
            "image 348/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0023_jpg.rf.0d732160a8e7e27556643a1163033708.jpg: 800x800 1 Closed-Book, 2 Worksheets, 10.6ms\n",
            "image 349/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0032_jpg.rf.286856dc427d0f968adcc9b39a93cfaf.jpg: 800x800 3 Closed-Books, 1 No-Book, 10.5ms\n",
            "image 350/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0034_jpg.rf.2c15ab50aea86d51ccbf5420afbada75.jpg: 800x800 3 Closed-Books, 1 No-Book, 10.5ms\n",
            "image 351/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0035_jpg.rf.82ed90acf1d7c9d426407b00458fd675.jpg: 800x800 3 Closed-Books, 1 No-Book, 10.5ms\n",
            "image 352/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0052_jpg.rf.a6deec8dc49be777dd7e6d7400f025e9.jpg: 800x800 2 Worksheets, 10.6ms\n",
            "image 353/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0053_jpg.rf.3fce5f6e4aa4c9b7f7f2f6831fe82dea.jpg: 800x800 1 Closed-Book, 4 Raising-Hands, 3 Worksheets, 10.5ms\n",
            "image 354/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0054_jpg.rf.77b43a0d2c060e71312b24d4787ff679.jpg: 800x800 1 Closed-Book, 2 Raising-Hands, 3 Worksheets, 11.6ms\n",
            "image 355/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0055_jpg.rf.4cd3b4cccba98e8d49b98ae88ecc0784.jpg: 800x800 1 Closed-Book, 2 Raising-Hands, 3 Worksheets, 11.3ms\n",
            "image 356/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0056_jpg.rf.079b5160ce28318ab99e0c84d7893bab.jpg: 800x800 1 Closed-Book, 2 Raising-Hands, 3 Worksheets, 10.6ms\n",
            "image 357/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0057_jpg.rf.458b6fa2fcffec056abb005d591bb4cc.jpg: 800x800 4 Closed-Books, 1 No-Book, 1 Opened-Book, 10.4ms\n",
            "image 358/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0058_jpg.rf.995d5a81640a4e21bb8f642a40a439e6.jpg: 800x800 2 Closed-Books, 1 No-Book, 1 Opened-Book, 1 Raising-Hand, 10.5ms\n",
            "image 359/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0059_jpg.rf.64fb1248f633c61f70937ea3e40e1783.jpg: 800x800 3 Closed-Books, 1 No-Book, 1 Opened-Book, 10.5ms\n",
            "image 360/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0061_jpg.rf.ac3607a1aa1ee01b93de3ede6e8f513f.jpg: 800x800 1 Closed-Book, 4 Raising-Hands, 2 Worksheets, 10.4ms\n",
            "image 361/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0062_jpg.rf.621c626cafda1e7e9836bab839563862.jpg: 800x800 1 Closed-Book, 4 Raising-Hands, 3 Worksheets, 10.5ms\n",
            "image 362/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0064_jpg.rf.c8728b5e934ddcf7f5275360bede3fa7.jpg: 800x800 1 Closed-Book, 5 Raising-Hands, 3 Worksheets, 10.6ms\n",
            "image 363/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0069_jpg.rf.a27f71620244da0d23bfe7962e0f8c29.jpg: 800x800 1 Closed-Book, 3 Raising-Hands, 1 Teacher-Explains, 3 Worksheets, 10.7ms\n",
            "image 364/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0071_jpg.rf.fe81edc4e60fffc00ce7a5d16933a1c7.jpg: 800x800 1 Closed-Book, 2 Worksheets, 10.7ms\n",
            "image 365/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0078_jpg.rf.3da6ca8441b0bcce3e2d9fe45e91978e.jpg: 800x800 1 Closed-Book, 6 Raising-Hands, 3 Worksheets, 11.0ms\n",
            "image 366/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0079_jpg.rf.b6edd336ef4b18030c49e46007d61344.jpg: 800x800 1 Closed-Book, 7 Raising-Hands, 2 Worksheets, 11.5ms\n",
            "image 367/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0081_jpg.rf.57b331b76651717bf4fda28b2008742d.jpg: 800x800 4 Closed-Books, 1 No-Book, 1 Opened-Book, 2 Raising-Hands, 10.7ms\n",
            "image 368/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0082_jpg.rf.8254f450e63c2bdedf30acddbe160f9f.jpg: 800x800 4 Closed-Books, 1 No-Book, 1 Opened-Book, 2 Raising-Hands, 10.9ms\n",
            "image 369/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0086_jpg.rf.56c7156f274fc738a3e3fbb97bbaa569.jpg: 800x800 1 Closed-Book, 3 Worksheets, 10.9ms\n",
            "image 370/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0090_jpg.rf.4d65cc13c83e1cbd59b39e9c17aae165.jpg: 800x800 1 Closed-Book, 1 Raising-Hand, 3 Worksheets, 10.8ms\n",
            "image 371/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0091_jpg.rf.c6f18813c21985e3670274f2254c8379.jpg: 800x800 1 Closed-Book, 3 Raising-Hands, 3 Worksheets, 10.7ms\n",
            "image 372/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0092_jpg.rf.0471a27433cfdcb2f2a470d7ef276686.jpg: 800x800 3 Closed-Books, 1 No-Book, 1 Opened-Book, 10.7ms\n",
            "image 373/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0093_jpg.rf.89107e10f26aebd34963bf2439d25e78.jpg: 800x800 5 Closed-Books, 1 No-Book, 1 Opened-Book, 1 Student-Answers, 10.8ms\n",
            "image 374/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0095_jpg.rf.9f127f2026a3ef2e78a026e62f500097.jpg: 800x800 4 Closed-Books, 1 No-Book, 1 Opened-Book, 1 Student-Answers, 10.6ms\n",
            "image 375/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0096_jpg.rf.6e6f7797469a6798f416e1ba0ad88123.jpg: 800x800 5 Closed-Books, 1 No-Book, 1 Opened-Book, 1 Student-Answers, 10.6ms\n",
            "image 376/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0098_jpg.rf.dc644044f8b117b5f0c9920a0f4d4fc2.jpg: 800x800 4 Closed-Books, 1 No-Book, 1 Opened-Book, 11.9ms\n",
            "image 377/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0099_jpg.rf.08f7acb06393a5c9131608dffbc58423.jpg: 800x800 2 Closed-Books, 1 No-Book, 1 Opened-Book, 13.4ms\n",
            "image 378/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0102_jpg.rf.d607aabe563f1ff315c9b731cf70f7f0.jpg: 800x800 3 Closed-Books, 1 No-Book, 1 Opened-Book, 13.3ms\n",
            "image 379/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0107_jpg.rf.e7a54036ca41d326984b7cbe1bae27eb.jpg: 800x800 1 Closed-Book, 1 Raising-Hand, 2 Worksheets, 13.2ms\n",
            "image 380/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0111_jpg.rf.1ae08bb8374fe5234ecabe7a678c381a.jpg: 800x800 1 Closed-Book, 6 Raising-Hands, 3 Worksheets, 13.1ms\n",
            "image 381/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0120_jpg.rf.707717a9e4ef355f0ec084156a5bdddd.jpg: 800x800 2 Closed-Books, 1 No-Book, 1 Opened-Book, 13.0ms\n",
            "image 382/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0121_jpg.rf.fc7c90c9a27043a211841c14f0f283cb.jpg: 800x800 2 Closed-Books, 1 No-Book, 13.1ms\n",
            "image 383/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0122_jpg.rf.a0be041069c97c4052b03feee6137d11.jpg: 800x800 3 Closed-Books, 1 No-Book, 1 Opened-Book, 13.1ms\n",
            "image 384/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0125_jpg.rf.61b35238c8c1c596da07eaa1b8f82992.jpg: 800x800 3 Closed-Books, 1 No-Book, 1 Opened-Book, 10.5ms\n",
            "image 385/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0126_jpg.rf.e6e7fdf3bc15d57df72f330054962b09.jpg: 800x800 3 Closed-Books, 1 No-Book, 1 Opened-Book, 1 Raising-Hand, 10.6ms\n",
            "image 386/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0129_jpg.rf.26e5a8b0d3234bebcde795df39246877.jpg: 800x800 1 Closed-Book, 3 Worksheets, 11.3ms\n",
            "image 387/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0130_jpg.rf.b2b9d260e5f482fe80da4e99fa06954f.jpg: 800x800 1 Closed-Book, 1 Raising-Hand, 3 Worksheets, 10.3ms\n",
            "image 388/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0146_jpg.rf.acc39f3102ffb40ebdb675089181e221.jpg: 800x800 4 Closed-Books, 1 No-Book, 1 Opened-Book, 10.4ms\n",
            "image 389/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0147_jpg.rf.78fc7043a2678095fd02c0d1b2c8b3b0.jpg: 800x800 4 Closed-Books, 1 No-Book, 1 Opened-Book, 1 Raising-Hand, 10.8ms\n",
            "image 390/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0149_jpg.rf.f676ac9ccb1296f38df2010b71a258cf.jpg: 800x800 1 Closed-Book, 3 Raising-Hands, 3 Worksheets, 10.5ms\n",
            "image 391/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0152_jpg.rf.773c15aa7c76a4e839f874f66b26b440.jpg: 800x800 3 Closed-Books, 2 Raising-Hands, 1 Teacher-Follows-up-Students, 10.5ms\n",
            "image 392/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0159_jpg.rf.b70ba026f5e06d66c8c29b5184c3fc5a.jpg: 800x800 2 Closed-Books, 1 No-Book, 1 Opened-Book, 1 Raising-Hand, 10.5ms\n",
            "image 393/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0167_jpg.rf.e6c68ff1cc373e03d165827d0b2f8fea.jpg: 800x800 1 Closed-Book, 1 Raising-Hand, 3 Worksheets, 10.5ms\n",
            "image 394/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0169_jpg.rf.11b62a6fc677d795ac15794c8b30f031.jpg: 800x800 1 Closed-Book, 2 Raising-Hands, 3 Worksheets, 10.4ms\n",
            "image 395/452 /content/datasets/students-behaviors-detection-4/test/images/video1173355343_mp4-0176_jpg.rf.a21118fc9d0d9810acaa8d6dddb2b269.jpg: 800x800 4 Closed-Books, 1 No-Book, 1 Opened-Book, 1 Raising-Hand, 1 Teacher-Follows-up-Students, 10.5ms\n",
            "image 396/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0003_jpg.rf.db0969c78fa9394e0c5b303fbc4468e7.jpg: 800x800 1 Teacher-Explains, 10.4ms\n",
            "image 397/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0010_jpg.rf.1ab5434c3e5d27f812db14e7915422c2.jpg: 800x800 4 Closed-Books, 10.7ms\n",
            "image 398/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0015_jpg.rf.b771763b5b035f9d79bc289375de6167.jpg: 800x800 2 Closed-Books, 11.2ms\n",
            "image 399/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0056_jpg.rf.04c9d096e416cf8c38b063aaba26dbb1.jpg: 800x800 2 Opened-Books, 3 Student-Readss, 10.6ms\n",
            "image 400/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0059_jpg.rf.946d50d0341bfed30422bb4e6daeb2dd.jpg: 800x800 3 Opened-Books, 3 Student-Readss, 10.6ms\n",
            "image 401/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0062_jpg.rf.a3d97fb461926e04d4dcff9b621afb42.jpg: 800x800 3 Opened-Books, 3 Student-Readss, 10.4ms\n",
            "image 402/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0069_jpg.rf.77c8d480db47b011458db48e40bff1ef.jpg: 800x800 1 No-Book, 1 Opened-Book, 1 Student-Reads, 1 Teacher-Explains, 10.5ms\n",
            "image 403/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0070_jpg.rf.3ef341a2373b9a1972d03498f28c10ed.jpg: 800x800 1 No-Book, 1 Opened-Book, 1 Student-Reads, 1 Student-Writes, 12.1ms\n",
            "image 404/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0101_jpg.rf.4bac09a4815a50fbeab0529cb3b69753.jpg: 800x800 3 Opened-Books, 1 Student-Writes, 10.6ms\n",
            "image 405/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0103_jpg.rf.3ba91dd2a006a6c2323340fd2bf97f7b.jpg: 800x800 3 Opened-Books, 10.6ms\n",
            "image 406/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0165_jpg.rf.5176d2f9c5611e7d2cc0c861314d9847.jpg: 800x800 2 Raising-Hands, 10.5ms\n",
            "image 407/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0168_jpg.rf.aae87c9820a23538c781d489f522cf41.jpg: 800x800 4 Opened-Books, 1 Student-Reads, 10.5ms\n",
            "image 408/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0171_jpg.rf.9153087b104e36fc40cfdf98d0f3f4c9.jpg: 800x800 4 Opened-Books, 1 Student-Answers, 10.5ms\n",
            "image 409/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0212_jpg.rf.18a015664ef6fffa795c4a611323fc0f.jpg: 800x800 1 Student-Answers, 10.4ms\n",
            "image 410/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0218_jpg.rf.3c5faa5aa52cdc04fe08119c91af38ab.jpg: 800x800 1 Teacher-Explains, 11.5ms\n",
            "image 411/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0226_jpg.rf.d09262bda15f450214e17024d4fff65c.jpg: 800x800 1 Teacher-Explains, 10.5ms\n",
            "image 412/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0235_jpg.rf.57e6dbaf685ee1a4e5cc3182566eed93.jpg: 800x800 1 Student-Answers, 10.5ms\n",
            "image 413/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0247_jpg.rf.8e5c245ba173e67037bbc548f9447a16.jpg: 800x800 1 Teacher-Explains, 10.4ms\n",
            "image 414/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0248_jpg.rf.676cb56edd602bdceb9ab24a4c588289.jpg: 800x800 1 Teacher-Explains, 10.4ms\n",
            "image 415/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0284_jpg.rf.8399a150c2ced7f135600c8cba33eb33.jpg: 800x800 1 Teacher-Explains, 10.4ms\n",
            "image 416/452 /content/datasets/students-behaviors-detection-4/test/images/video1211071788_mp4-0286_jpg.rf.b4bf20b6124f432124e5e9b14b712744.jpg: 800x800 2 Teacher-Explainss, 10.5ms\n",
            "image 417/452 /content/datasets/students-behaviors-detection-4/test/images/video1753010070_mp4-0004_jpg.rf.065e3c76d1d1256afbdaf5aa10549533.jpg: 800x800 2 Closed-Books, 3 Opened-Books, 2 Worksheets, 10.5ms\n",
            "image 418/452 /content/datasets/students-behaviors-detection-4/test/images/video1753010070_mp4-0017_jpg.rf.99d9abc2039e96b5d32497a3627eb8b6.jpg: 800x800 1 Teacher-Explains, 10.6ms\n",
            "image 419/452 /content/datasets/students-behaviors-detection-4/test/images/video1753010070_mp4-0018_jpg.rf.7a6c0cf7e4b85692a1a38b66e84db77e.jpg: 800x800 1 Teacher-Explains, 10.6ms\n",
            "image 420/452 /content/datasets/students-behaviors-detection-4/test/images/video1753010070_mp4-0031_jpg.rf.b8d52552e61f53b5327c48815b92d41e.jpg: 800x800 2 Closed-Books, 3 Opened-Books, 1 Worksheet, 10.5ms\n",
            "image 421/452 /content/datasets/students-behaviors-detection-4/test/images/video1753010070_mp4-0037_jpg.rf.1f8db5166901c6e818ec1e01b55eb53d.jpg: 800x800 6 Opened-Books, 1 Worksheet, 10.5ms\n",
            "image 422/452 /content/datasets/students-behaviors-detection-4/test/images/video1753010070_mp4-0062_jpg.rf.9247b7ad21d758cfb2bb75b3fcc64de4.jpg: 800x800 6 Opened-Books, 1 Worksheet, 10.5ms\n",
            "image 423/452 /content/datasets/students-behaviors-detection-4/test/images/video1753010070_mp4-0101_jpg.rf.59c5a69214489528de08a962b8ca1a5f.jpg: 800x800 3 Opened-Books, 1 Student-Answers, 1 Worksheet, 11.0ms\n",
            "image 424/452 /content/datasets/students-behaviors-detection-4/test/images/video1753010070_mp4-0114_jpg.rf.342481a40426d8ceea5b4bcacd3789a3.jpg: 800x800 1 Closed-Book, 3 Opened-Books, 1 Student-Answers, 1 Worksheet, 10.6ms\n",
            "image 425/452 /content/datasets/students-behaviors-detection-4/test/images/video1753010070_mp4-0166_jpg.rf.b28e1f640c72396f6bd8021a2b89ca8a.jpg: 800x800 1 Student-Answers, 10.7ms\n",
            "image 426/452 /content/datasets/students-behaviors-detection-4/test/images/video1753010070_mp4-0177_jpg.rf.80b2d277884c6dde9f3c3660e3cb686c.jpg: 800x800 1 Closed-Book, 2 Opened-Books, 1 Worksheet, 10.7ms\n",
            "image 427/452 /content/datasets/students-behaviors-detection-4/test/images/video1753010070_mp4-0204_jpg.rf.e25ee876c41e67551caf0b00e1db5415.jpg: 800x800 1 Closed-Book, 2 Opened-Books, 1 Student-Answers, 1 Worksheet, 10.5ms\n",
            "image 428/452 /content/datasets/students-behaviors-detection-4/test/images/video1753010070_mp4-0211_jpg.rf.ae45bb99e4f90cf88eaaf931c97a480e.jpg: 800x800 1 Closed-Book, 2 Opened-Books, 1 Student-Answers, 1 Worksheet, 10.6ms\n",
            "image 429/452 /content/datasets/students-behaviors-detection-4/test/images/video1753010070_mp4-0223_jpg.rf.d8e9f4b1a18fc648ee7af1489c66d594.jpg: 800x800 1 Teacher-Explains, 10.5ms\n",
            "image 430/452 /content/datasets/students-behaviors-detection-4/test/images/video1753010070_mp4-0240_jpg.rf.0f5112f493b9be906f39db0c2cb4c38e.jpg: 800x800 4 Opened-Books, 1 Worksheet, 10.5ms\n",
            "image 431/452 /content/datasets/students-behaviors-detection-4/test/images/video1753010070_mp4-0242_jpg.rf.20e555efa37c5bf37a394510680358b7.jpg: 800x800 1 Closed-Book, 3 Opened-Books, 1 Worksheet, 10.8ms\n",
            "image 432/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0015_jpg.rf.53478817ce3fe4f406a8eec6a2cc56e3.jpg: 800x800 1 Closed-Book, 2 Opened-Books, 2 Raising-Hands, 10.3ms\n",
            "image 433/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0022_jpg.rf.6329e9f0228c2ec7695482cbfc737a7d.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 1 Raising-Hand, 1 Teacher-Explains, 10.5ms\n",
            "image 434/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0040_jpg.rf.b7b875bb0a475dc2ecd533910e390ddf.jpg: 800x800 1 Closed-Book, 2 Opened-Books, 1 Teacher-Explains, 10.4ms\n",
            "image 435/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0061_jpg.rf.7d07ed62b1688d0bddec43a7bfb08477.jpg: 800x800 1 Opened-Book, 2 Raising-Hands, 10.4ms\n",
            "image 436/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0076_jpg.rf.2cf432d8f6bd578098afed25503b6f9d.jpg: 800x800 1 Closed-Book, 2 Opened-Books, 10.7ms\n",
            "image 437/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0077_jpg.rf.498f3b4724e02803334104f2220b441a.jpg: 800x800 1 Closed-Book, 2 Opened-Books, 10.4ms\n",
            "image 438/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0085_jpg.rf.e451799a693efadce510f7c502640c18.jpg: 800x800 1 Closed-Book, 2 Opened-Books, 1 Teacher-Explains, 10.4ms\n",
            "image 439/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0099_jpg.rf.645b76950272a6e2b210cc05d1198329.jpg: 800x800 1 Closed-Book, 2 Opened-Books, 2 Raising-Hands, 10.5ms\n",
            "image 440/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0100_jpg.rf.58dd5bed4efb3748d83c30f5aefa1e1f.jpg: 800x800 1 Closed-Book, 2 Opened-Books, 1 Teacher-Explains, 10.4ms\n",
            "image 441/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0108_jpg.rf.0c541d19b8233f7a8a1dbf88e0d46090.jpg: 800x800 1 Closed-Book, 2 Opened-Books, 1 Raising-Hand, 10.3ms\n",
            "image 442/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0131_jpg.rf.72fd3f904b98e65744c45283614bbf98.jpg: 800x800 1 Closed-Book, 1 Raising-Hand, 12.9ms\n",
            "image 443/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0135_jpg.rf.206c6ad8312234f3390bbff20629cf53.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 10.4ms\n",
            "image 444/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0138_jpg.rf.075741826693b425fba551f582c3d12f.jpg: 800x800 1 Closed-Book, 1 Teacher-Explains, 10.3ms\n",
            "image 445/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0141_jpg.rf.91792f69f36d8de6b5deb2b50d4f3a3e.jpg: 800x800 1 Closed-Book, 2 Opened-Books, 1 Raising-Hand, 1 Teacher-Explains, 11.9ms\n",
            "image 446/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0147_jpg.rf.df1c917201500cca1c48bdc0576f8e09.jpg: 800x800 1 Closed-Book, 2 Opened-Books, 2 Raising-Hands, 1 Teacher-Explains, 13.8ms\n",
            "image 447/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0157_jpg.rf.825df8d7d583cfb6fe4efb301cb881f1.jpg: 800x800 2 Opened-Books, 3 Raising-Hands, 13.4ms\n",
            "image 448/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0187_jpg.rf.f8dc8356e1e7e19d562c62b2d507b428.jpg: 800x800 1 Closed-Book, 1 Opened-Book, 1 Raising-Hand, 14.0ms\n",
            "image 449/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0225_jpg.rf.37b478a19d15653d40931632d1ff9317.jpg: 800x800 1 Closed-Book, 1 Raising-Hand, 1 Teacher-Explains, 13.5ms\n",
            "image 450/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0232_jpg.rf.4e74f41ca1b67f19891c2dc760868c84.jpg: 800x800 2 Opened-Books, 13.7ms\n",
            "image 451/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0241_jpg.rf.cc42d4db52d9f3652c3254ef61ca799f.jpg: 800x800 2 Closed-Books, 1 Opened-Book, 1 Teacher-Explains, 13.4ms\n",
            "image 452/452 /content/datasets/students-behaviors-detection-4/test/images/video1927420561_mp4-0270_jpg.rf.58dbd80f30bdc7888c74e9d0762a72d0.jpg: 800x800 1 Opened-Book, 1 Student-Writes, 1 Teacher-Explains, 13.5ms\n",
            "Speed: 2.8ms preprocess, 10.8ms inference, 2.6ms postprocess per image at shape (1, 3, 800, 800)\n",
            "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
            "üí° Learn more at https://docs.ultralytics.com/modes/predict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualize Inference Results**"
      ],
      "metadata": {
        "id": "Tm-jTeJBk8fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Define the base path where the folders are located\n",
        "base_path = '/content/runs/detect/'\n",
        "\n",
        "# List all directories that start with 'predict' in the base path\n",
        "subfolders = [os.path.join(base_path, d) for d in os.listdir(base_path)\n",
        "              if os.path.isdir(os.path.join(base_path, d)) and d.startswith('predict2')]\n",
        "\n",
        "# Find the latest folder by modification time\n",
        "latest_folder = max(subfolders, key=os.path.getmtime)\n",
        "\n",
        "image_paths = glob.glob(f'{latest_folder}/*.jpg')[:5]\n",
        "\n",
        "# Display each image\n",
        "for image_path in image_paths:\n",
        "    display(Image(filename=image_path, width=600))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "rgNsZEbZk8Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Deploy Model on Roboflow**"
      ],
      "metadata": {
        "id": "B6BDtaeZnLlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "project.version(dataset.version).deploy(model_type=\"yolov8\", model_path=f\"{HOME}/runs/detect/train/\")"
      ],
      "metadata": {
        "id": "hEsHxCkZnRM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference on your model on a persistent, auto-scaling, cloud API\n",
        "\n",
        "# Load model\n",
        "model = project.version(dataset.version).model\n",
        "assert model, \"Model deployment is still loading\"\n",
        "\n",
        "# Choose a random test image\n",
        "import os, random\n",
        "test_set_loc = dataset.location + \"/test/images/\"\n",
        "random_test_image = random.choice(os.listdir(test_set_loc))\n",
        "print(\"running inference on \" + random_test_image)\n",
        "\n",
        "pred = model.predict(test_set_loc + random_test_image, confidence=40, overlap=30).json()\n",
        "pred"
      ],
      "metadata": {
        "id": "uWZnDCBgnhUI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}